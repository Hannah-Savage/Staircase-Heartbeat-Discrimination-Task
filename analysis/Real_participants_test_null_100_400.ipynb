{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d717e633-cde0-4953-b8de-fff7cbb4a26f",
   "metadata": {},
   "source": [
    "# Set env:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f601df1-0259-4164-aead-f0683858253f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Patch\n",
    "import re\n",
    "import scipy\n",
    "from scipy.stats import norm, linregress, pearsonr\n",
    "from scipy.optimize import Bounds, LinearConstraint, minimize, SR1\n",
    "import seaborn as sns\n",
    "import sys\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from IPython.display import display, HTML\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, roc_curve, auc\n",
    "from tqdm import tqdm \n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "\n",
    "# Define fixed ranges for the structure\n",
    "pred_pss_values = np.arange(100, 401, 50)  # Pred_PSS_Value: 0 to 400, increments of 50\n",
    "pred_accuracies = np.arange(0, 1.1, 0.1).round(1)  # Pred_Accuracy: 0.1 to 1.0, increments of 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4c40c1-971d-4f26-9d79-739c82996a52",
   "metadata": {},
   "source": [
    "# Define functions:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd1bab6",
   "metadata": {},
   "source": [
    "REFERENCE SAMPLE: Function to compute response counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ea580b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_response_counts_ref(participants_df, participant_id, i):\n",
    "    # Count the occurrences of each response code per Staircase_name\n",
    "    response_counts = participants_df.groupby(['Staircase_name', 'Response_code']).size().unstack(fill_value=0)\n",
    "    \n",
    "    # Rename columns for clarity\n",
    "    response_counts = response_counts.rename(columns={-1: \"Before_Response\", \n",
    "                                                       0: \"Same_Response\", \n",
    "                                                       1: \"After_Response\"})\n",
    "    \n",
    "    # Reset index for merging or visualization\n",
    "    response_counts = response_counts.reset_index()\n",
    "    \n",
    "    # Add participant_id column\n",
    "    response_counts['participant_id'] = f\"{participant_id}_{i}\"\n",
    "    \n",
    "    return response_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df06e18-e740-4a6a-8979-cf840a2370bd",
   "metadata": {},
   "source": [
    "REFERENCE SAMPLE: Function to create simuation_matrix. Creates plots of Point of subjective simultaneity (PSS) and accuracy trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39e8030-e2b8-4ef5-9ace-615d5af1e563",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def reference_matrix(simulation_df, pred_pss_values, pred_accuracies, drop = True, plots = True):\n",
    "    \n",
    "    # Recode trial numbers: Make trials continuous from 1-30\n",
    "    simulation_df['Recode_Trial'] = np.where(simulation_df['Staircase_name'].str.contains('400'),\n",
    "                                             simulation_df['Trial'], \n",
    "                                             simulation_df['Trial'] + 15)\n",
    "    \n",
    "    \n",
    "    if plots:   \n",
    "        colors = plt.cm.viridis(np.linspace(0, 1, len(pred_accuracies)))  # Color map for accuracy levels\n",
    "\n",
    "        # Create a grid for the subplots\n",
    "        n_rows = 4\n",
    "        n_cols = 2\n",
    "        n_iterations = 100\n",
    "        fig, axs = plt.subplots(n_rows, n_cols, figsize=(12, 12), dpi=300)\n",
    "        axs = axs.ravel()  # Flatten the 2D array of axes to 1D for easy iteration\n",
    "\n",
    "        for i, pss_value in enumerate(pred_pss_values):\n",
    "            ax = axs[i]\n",
    "            df_pss = simulation_df[simulation_df['PSS_value'] == pss_value]\n",
    "\n",
    "            # Set title for the subplot\n",
    "            ax.set_title(f'PSS Value: {pss_value} ms', fontsize=14)\n",
    "\n",
    "            # Loop through each accuracy value\n",
    "            for accuracy_idx, accuracy in enumerate(pred_accuracies):\n",
    "                # Filter data by accuracy and group by Staircase_name and Trial\n",
    "                df_accuracy = df_pss[df_pss['Accuracy'] == accuracy]\n",
    "                mean_stim_values = df_accuracy.groupby(['Staircase_name', 'Trial'])['Current Delay'].mean()\n",
    "                std_stim_values = df_accuracy.groupby(['Staircase_name', 'Trial'])['Current Delay'].std()\n",
    "\n",
    "                # Plot each staircase separately\n",
    "                for staircase_name, group in mean_stim_values.groupby(level=0):\n",
    "                    # Compute 95% confidence intervals\n",
    "                    trial_numbers = group.index.get_level_values(1)\n",
    "                    ci_95 = 1.96 * std_stim_values.loc[staircase_name] / np.sqrt(n_iterations)\n",
    "\n",
    "                    # Plot the mean stimulus values and confidence intervals\n",
    "                    ax.plot(group.index.get_level_values(1), group.values, \n",
    "                            label=f'{staircase_name} - Accuracy {accuracy*100:.0f}%', \n",
    "                            color=colors[accuracy_idx], \n",
    "                            marker='o')\n",
    "\n",
    "                    ax.fill_between(group.index.get_level_values(1), \n",
    "                                    group.values - ci_95, \n",
    "                                    group.values + ci_95, \n",
    "                                    color=colors[accuracy_idx], \n",
    "                                    alpha=0.2)\n",
    "\n",
    "            # Labeling the subplot\n",
    "            ax.set_xlabel('Trial')\n",
    "            ax.set_ylabel('Current Delay (ms)')\n",
    "            ax.set_ylim(0, 800)  # Set y-axis limit between 0 and 800\n",
    "            #ax.legend(loc='upper right')\n",
    "            ax.axhspan((pss_value-1), (pss_value +1), color='red', alpha=1.0, linestyle = '--', zorder=10)\n",
    "\n",
    "            # Adjust layout for better spacing between subplots\n",
    "            plt.tight_layout()\n",
    "\n",
    "            \n",
    "            save_path = '/PATH/Reference_trajectories_100_400.png'\n",
    "            \n",
    "        plt.savefig(save_path) \n",
    "        plt.show()\n",
    "        print(f\"Saved to {save_path}\")\n",
    "    \n",
    "    if drop: \n",
    "        #Remove the first two trials from each staircase\n",
    "        #simulation_df = simulation_df[~simulation_df['Recode_Trial'].isin([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25])] #last 5 trials only\n",
    "        simulation_df = simulation_df[~simulation_df['Recode_Trial'].isin([1, 2, 3, 4, 5, 16, 17, 18, 19, 20])] #trials 5-15\n",
    "\n",
    "    \n",
    "    # Create a pivot table to calculate mean delay\n",
    "    pivot_df = simulation_df.pivot_table(\n",
    "        index=['PSS_value', 'Recode_Trial'],  # Depth: PSS_value; Rows: Trial number\n",
    "        columns='Accuracy',                  # Columns: Accuracy values\n",
    "        values='Current Delay',              # Values: Mean Delay\n",
    "        aggfunc='mean'                       # Aggregate function: mean\n",
    "    )\n",
    "    \n",
    "    # Only use PSS = 100-400; exclude PSS isin([50, 450, 500, 550, 600]\n",
    "    pivot_df = pivot_df.loc[~pivot_df.index.get_level_values('PSS_value').isin([50, 450, 500, 550, 600])]\n",
    "    \n",
    "    # Fill missing values\n",
    "    pivot_df = pivot_df.fillna(0)\n",
    "    \n",
    "    # Prepare labels for heatmaps\n",
    "    trial_labels = [f\"Trial {i}\" for i in sorted(simulation_df['Recode_Trial'].unique())]\n",
    "    accuracy_labels = [f\"Acc {round(acc, 2)}\" for acc in sorted(simulation_df['Accuracy'].unique())]\n",
    "    pss_values = sorted(pivot_df.index.get_level_values('PSS_value').unique())\n",
    "    #pss_values = sorted(simulation_df[~simulation_df['PSS_value'].isin([50, 450, 500, 550, 600])\n",
    "\n",
    "    return pivot_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9717dae-e5cf-4e48-b583-13ee96d0dce1",
   "metadata": {
    "tags": []
   },
   "source": [
    "REAL SAMPLE: Function to process and plot data for each PSS value, grouped by Staircase_name and Accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ee6571-4d1e-4810-bfba-a8501012ecc0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_simulations_with_participant_data(simulation_df, participant_df, participant_id, pss_value_list, accuracy_list, n_iterations, trials_per_iteration=15):\n",
    "    colors = plt.cm.viridis(np.linspace(0, 1, len(accuracy_list)))  # Color map for accuracy levels\n",
    "\n",
    "    # Create a grid for the subplots\n",
    "    n_rows = 3\n",
    "    n_cols = 3\n",
    "    n_iterations = 100\n",
    "    fig, axs = plt.subplots(n_rows, n_cols, figsize=(18, 12))\n",
    "    axs = axs.ravel()  # Flatten the 2D array of axes to 1D for easy iteration\n",
    "\n",
    "    # Loop through each PSS value\n",
    "    for i, pss_value in enumerate(pss_value_list):\n",
    "        ax = axs[i]\n",
    "        df_pss = simulation_df[simulation_df['PSS_value'] == pss_value]\n",
    "        \n",
    "        # Set title for the subplot\n",
    "        ax.set_title(f'PSS Value: {pss_value} ms', fontsize=14)\n",
    "\n",
    "        # Loop through each accuracy value\n",
    "        for accuracy_idx, accuracy in enumerate(accuracy_list):\n",
    "            # Filter data by accuracy and group by Staircase_name and Trial\n",
    "            df_accuracy = df_pss[df_pss['Accuracy'] == accuracy]\n",
    "            mean_stim_values = df_accuracy.groupby(['Staircase_name', 'Trial'])['Current Delay'].mean()\n",
    "            std_stim_values = df_accuracy.groupby(['Staircase_name', 'Trial'])['Current Delay'].std()\n",
    "\n",
    "            # Plot each staircase separately\n",
    "            for staircase_name, group in mean_stim_values.groupby(level=0):\n",
    "                # Compute 95% confidence intervals\n",
    "                trial_numbers = group.index.get_level_values(1)\n",
    "                ci_95 = 1.96 * std_stim_values.loc[staircase_name] / np.sqrt(n_iterations)\n",
    "\n",
    "                # Plot the mean stimulus values and confidence intervals\n",
    "                ax.plot(group.index.get_level_values(1), group.values, \n",
    "                        label=f'{staircase_name} - Accuracy {accuracy*100:.0f}%', \n",
    "                        color=colors[accuracy_idx], \n",
    "                        marker='o')\n",
    "\n",
    "                ax.fill_between(group.index.get_level_values(1), \n",
    "                                group.values - ci_95, \n",
    "                                group.values + ci_95, \n",
    "                                color=colors[accuracy_idx], \n",
    "                                alpha=0.2)\n",
    "\n",
    "        # Overlay participant data (mean of repeated staircases)\n",
    "        # Group participant data by Modified Staircase_name and Trial\n",
    "        participant_mean_data = participant_df.groupby(['Modified_Staircase_name', 'Trial'])['Current Delay'].mean()\n",
    "        participant_std_data = participant_df.groupby(['Modified_Staircase_name', 'Trial'])['Current Delay'].std()\n",
    "\n",
    "        # Plot each staircase mean with a different line\n",
    "        for staircase_name in participant_df['Modified_Staircase_name'].unique():\n",
    "            # Filter participant data for the current staircase\n",
    "            participant_data_for_staircase = participant_mean_data.loc[staircase_name]\n",
    "            if len(participant_data_for_staircase) > 1:  # Ensure there is more than 1 data point\n",
    "                trial_numbers = participant_data_for_staircase.index  # X-axis: Trial numbers\n",
    "                mean_delays = participant_data_for_staircase.values  # Y-axis: Current Delay values\n",
    "\n",
    "                # Compute confidence intervals for participant data\n",
    "                ci_participant_95 = 1.96 * participant_std_data.loc[staircase_name] / np.sqrt(n_iterations)\n",
    "\n",
    "                # Plot the participant mean and confidence interval\n",
    "                ax.plot(trial_numbers, mean_delays, \n",
    "                        label=f'Participant ({staircase_name})', \n",
    "                        color='red', \n",
    "                        linewidth=2, \n",
    "                        marker='x')\n",
    "\n",
    "                ax.fill_between(trial_numbers, \n",
    "                                mean_delays - ci_participant_95, \n",
    "                                mean_delays + ci_participant_95, \n",
    "                                color='red', alpha=0.2)\n",
    "\n",
    "        # Labeling the subplot\n",
    "        ax.set_xlabel('Trial')\n",
    "        ax.set_ylabel('Current Delay (ms)')\n",
    "        ax.set_ylim(0, 800)  # Set y-axis limit between 0 and 800\n",
    "        #ax.legend(loc='upper right')\n",
    "\n",
    "    # Adjust layout and show the plot\n",
    "    file_name = '/PATH/Real_participants_trajectory_plots/Trajectory_sub-' + participant_id + '.png'\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(file_name)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21894cf6",
   "metadata": {},
   "source": [
    "REAL SAMPLE: Function to plot data for each PSS value, grouped by Staircase_name and Accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398b549d-64d8-4acf-924b-24b188c145a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_participant_trajectory(participant_df, participant_id, trials_per_iteration=15):\n",
    "    n_iterations = 100  # Used for confidence interval calculation\n",
    "    \n",
    "    # Create a single plot\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    # Loop through each unique staircase in 'Staircase_name'\n",
    "    for staircase_name in participant_df['Staircase_name'].unique():\n",
    "        # Filter participant data for the current staircase\n",
    "        staircase_data = participant_df[participant_df['Staircase_name'] == staircase_name]\n",
    "        \n",
    "        # Compute mean based on 'Modified_Staircase_name'\n",
    "        participant_mean_data = staircase_data.groupby(['Modified_Staircase_name', 'Trial'])['Current Delay'].mean()\n",
    "        participant_std_data = staircase_data.groupby(['Modified_Staircase_name', 'Trial'])['Current Delay'].std()\n",
    "        \n",
    "        for modified_staircase_name in staircase_data['Modified_Staircase_name'].unique():\n",
    "            if modified_staircase_name in participant_mean_data.index:\n",
    "                modified_staircase_data = participant_mean_data.loc[modified_staircase_name]\n",
    "                trial_numbers = modified_staircase_data.index  # X-axis: Trial numbers\n",
    "                mean_delays = modified_staircase_data.values  # Y-axis: Current Delay values\n",
    "                \n",
    "                # Compute confidence intervals for participant data\n",
    "                ci_participant_95 = 1.96 * participant_std_data.loc[modified_staircase_name] / np.sqrt(n_iterations)\n",
    "                \n",
    "                # Plot the participant mean and confidence interval\n",
    "                ax.plot(trial_numbers, mean_delays, \n",
    "                        label=f'Participant ({modified_staircase_name})', \n",
    "                        linewidth=2, \n",
    "                        color = 'blue',\n",
    "                        marker='x')\n",
    "                \n",
    "                ax.fill_between(trial_numbers, \n",
    "                                mean_delays - ci_participant_95, \n",
    "                                mean_delays + ci_participant_95, \n",
    "                                color = 'blue',\n",
    "                                alpha=0.2)\n",
    "\n",
    "        for staircase_name in participant_df['Modified_Staircase_name'].unique():\n",
    "            # Filter participant data for the current staircase\n",
    "            staircase_data = participant_df[participant_df['Modified_Staircase_name'] == staircase_name]\n",
    "            \n",
    "            # Compute mean based on 'Modified_Staircase_name'\n",
    "            participant_mean_data = staircase_data.groupby(['Modified_Staircase_name', 'Trial'])['Current Delay'].mean()\n",
    "            participant_std_data = staircase_data.groupby(['Modified_Staircase_name', 'Trial'])['Current Delay'].std()\n",
    "            \n",
    "            for modified_staircase_name in staircase_data['Modified_Staircase_name'].unique():\n",
    "                if modified_staircase_name in participant_mean_data.index:\n",
    "                    modified_staircase_data = participant_mean_data.loc[modified_staircase_name]\n",
    "                    trial_numbers = modified_staircase_data.index  # X-axis: Trial numbers\n",
    "                    mean_delays = modified_staircase_data.values  # Y-axis: Current Delay values\n",
    "                    \n",
    "                    # Compute confidence intervals for participant data\n",
    "                    ci_participant_95 = 1.96 * participant_std_data.loc[modified_staircase_name] / np.sqrt(n_iterations)\n",
    "                    \n",
    "                    # Plot the participant mean and confidence interval\n",
    "                    ax.plot(trial_numbers, mean_delays, \n",
    "                            label=f'Participant ({staircase_name})', \n",
    "                            color='red', \n",
    "                            linewidth=2, \n",
    "                            marker='x')\n",
    "    \n",
    "                    ax.fill_between(trial_numbers, \n",
    "                                    mean_delays - ci_participant_95, \n",
    "                                    mean_delays + ci_participant_95, \n",
    "                                    color='red', alpha=0.2)\n",
    "    \n",
    "    # Labeling the plot\n",
    "    ax.set_xlabel('Trial')\n",
    "    ax.set_ylabel('Current Delay (ms)')\n",
    "    ax.set_ylim(0, 800)  # Set y-axis limit between 0 and 800\n",
    "    #ax.legend(loc='upper right')\n",
    "    \n",
    "    # Save and show the plot\n",
    "    file_name = f'/PATH/Real_participants_trajectory_plots/Trajectory_nosim_sub-{participant_id}.png'\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(file_name)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed06122a-5271-4e5b-af7b-34a4134afdb5",
   "metadata": {
    "tags": []
   },
   "source": [
    "REAL SAMPLE: Function to generate 'participant_matrix'. For each participant returns pred_pss_value, pred_accuracy, rmse, rmse_table, and optional heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9f6d5e-dee4-43ed-9dcb-7660c35a91d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def participant_matrix(df, participant_id, pivot_df, drop = True, plots=False, verbose=False):\n",
    "    # Filter the data for this participant directly from the DataFrame\n",
    "    participant_data = df\n",
    "    \n",
    "    if participant_data.empty:\n",
    "        # If no data is found, return None\n",
    "        return None, None, None, None, None  # Return None if no data is found\n",
    "\n",
    "    # Recode the trial numbers: 1-15 for 400ms, 16-30 for 100ms\n",
    "    participant_data.loc[:, 'Recode_Trial'] = np.where(participant_data['Staircase_name'].str.contains('400'),\n",
    "                                                       participant_data['Trial'], \n",
    "                                                       participant_data['Trial'] + 15)\n",
    "\n",
    "    # For 400ms (trials 1-15)\n",
    "    mean_delays_400 = participant_data[participant_data['Staircase_name'].str.contains('400')] \\\n",
    "                        .groupby('Recode_Trial')['Current Delay'].mean()\n",
    "\n",
    "    # For 100ms (trials 16-30)\n",
    "    mean_delays_100 = participant_data[participant_data['Staircase_name'].str.contains('100')] \\\n",
    "                        .groupby('Recode_Trial')['Current Delay'].mean()\n",
    "\n",
    "    # Combine the two mean delay series (400_1, 400_2, and 100_1, 100_2) into one DataFrame\n",
    "    mean_delays = pd.concat([mean_delays_400, mean_delays_100])\n",
    "\n",
    "    # Reindex the trials: 1-15 for 400ms, 16-30 for 100ms\n",
    "    mean_delays.index = np.arange(1, 31)\n",
    "\n",
    "    if drop:\n",
    "        #mean_delays = mean_delays.drop([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]) # last 5 trials only\n",
    "        mean_delays = mean_delays.drop([1, 2, 3, 4, 5, 16, 17, 18, 19, 20]) #trials 5-15\n",
    "\n",
    "    if verbose: \n",
    "        print('Mean Delays:')\n",
    "        print(mean_delays)\n",
    "        \n",
    "    # Create an empty DataFrame to store the RMSE values for each PSS and accuracy level\n",
    "    rmse_table = pd.DataFrame(index=pivot_df.index.get_level_values('PSS_value').unique(), \n",
    "                              columns=[round(i * 0.1, 1) for i in range(11)])\n",
    "\n",
    "    # Loop through each PSS value and accuracy level, calculate RMSE and store it in the table\n",
    "    for pss in rmse_table.index:\n",
    "        for accuracy in rmse_table.columns:\n",
    "            # Extract the trial indices for the current PSS and accuracy level from pivot_df\n",
    "            trial_indices = pivot_df.xs(pss, level='PSS_value').loc[:, accuracy].index\n",
    "\n",
    "            # Extract corresponding mean delay values for those trials\n",
    "            mean_delay_values = mean_delays.loc[trial_indices].values\n",
    "\n",
    "            # Extract the corresponding pivot values for the current PSS and accuracy level\n",
    "            pivot_values = pivot_df.xs(pss, level='PSS_value').loc[:, accuracy].values\n",
    "\n",
    "            # Calculate RMSE if both arrays are not empty\n",
    "            if mean_delay_values.size > 0 and pivot_values.size > 0:\n",
    "                rmse = np.sqrt(np.mean((mean_delay_values - pivot_values) ** 2))\n",
    "            else:\n",
    "                rmse = np.nan  # If no data is available, assign NaN\n",
    "\n",
    "            # Store the RMSE in the table\n",
    "            rmse_table.loc[pss, accuracy] = rmse\n",
    "    \n",
    "    if verbose: \n",
    "        display(HTML(rmse_table.to_html()))\n",
    "\n",
    "    # Ensure the RMSE table is numeric (in case of any issues with non-numeric values)\n",
    "    rmse_table = rmse_table.apply(pd.to_numeric, errors='coerce')\n",
    "    \n",
    "        # 2. Find the lowest 5 RMSE values\n",
    "    lowest_rmse = rmse_table.unstack().sort_values().head(5)\n",
    "\n",
    "    # Display the lowest 5 RMSE values along with corresponding PSS and accuracy levels\n",
    "    lowest_rmse_table = pd.DataFrame(lowest_rmse).reset_index()\n",
    "    lowest_rmse_table.columns = ['Accuracy_Level', 'PSS_Value', 'RMSE']\n",
    "    \n",
    "    if verbose: \n",
    "        display(HTML(lowest_rmse_table.to_html()))\n",
    "        print(f\"{lowest_rmse_table[0:1]['PSS_Value']} \\n {lowest_rmse_table[0:1]['Accuracy_Level']} \\n {lowest_rmse_table[0:1]['RMSE']}\")\n",
    "\n",
    "    # Assuming that pred_pss_value, pred_accuracy, and rmse are the first values from lowest_rmse_table\n",
    "    pred_pss_value = lowest_rmse_table['PSS_Value'].values[0]\n",
    "    pred_accuracy = lowest_rmse_table['Accuracy_Level'].values[0]\n",
    "    rmse = lowest_rmse_table['RMSE'].values[0]\n",
    "    \n",
    "    \n",
    "    # Plot if requested\n",
    "    if plots:\n",
    "        # Create the heatmap\n",
    "        plt.figure(figsize=(12, 8))  # Adjust figure size if necessary\n",
    "        sns.heatmap(rmse_table, annot=True, cmap=\"coolwarm\", cbar_kws={'label': 'RMSE'}, fmt='.1f')\n",
    "        plt.title(\"Heatmap of RMSE for Different PSS Values and Accuracy Levels\")\n",
    "        plt.xlabel(\"Accuracy Levels\")\n",
    "        plt.ylabel(\"PSS Values\")\n",
    "        file_name = '/PATH/RMSE_heatmaps/PSS_stimulus_progression_sub-' + participant_id + '.png'\n",
    "        plt.savefig(file_name)\n",
    "        #plt.show()\n",
    "                \n",
    "            \n",
    "        #Create a density plot:\n",
    "        # Flatten the RMSE table values into a 1D array\n",
    "        rmse_values = rmse_table.values.flatten().round()\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        sns.kdeplot(rmse_values, shade=True, color='skyblue', bw_adjust=0.5)\n",
    "\n",
    "        # Add annotated vertical lines based on RMSE values from the table\n",
    "        # Assuming these are the RMSE values where you want the lines\n",
    "        rmse_table['RMSE'] = pd.to_numeric(lowest_rmse_table['RMSE'], errors='coerce')\n",
    "        rmse_line_values = lowest_rmse_table['RMSE'].tolist()\n",
    "\n",
    "        # Plot vertical lines and annotate\n",
    "        for value in rmse_line_values:\n",
    "            plt.axvline(x=value, color='red', linestyle='--', linewidth=1)\n",
    "            plt.text(value + 0.2, 0.002, f'RMSE = {value:.0f}', color='red', rotation=45)\n",
    "\n",
    "        # Add title and labels\n",
    "        plt.title(\"Density Plot of RMSE Values with Annotated Vertical Lines\")\n",
    "        plt.xlabel(\"RMSE Value\")\n",
    "        plt.ylabel(\"Density\")\n",
    "        # Display the plot\n",
    "        #plt.show()\n",
    "\n",
    "        \n",
    "        # Create a scatter plot\n",
    "        rmse_table_reset = rmse_table.reset_index()\n",
    "\n",
    "        # Melt the DataFrame so that each row corresponds to an (accuracy, RMSE) pair with a PSS value\n",
    "        rmse_table_melted = rmse_table_reset.melt(id_vars=['PSS_value'], var_name='Accuracy_Level', value_name='RMSE')\n",
    "\n",
    "        # Create the scatter plot\n",
    "        plt.figure(figsize=(10, 6))\n",
    "\n",
    "        # Use seaborn's scatterplot with color mapped to PSS_value\n",
    "        sns.scatterplot(data=rmse_table_melted, x='Accuracy_Level', y='RMSE', hue='PSS_value', palette='viridis', s=100, marker='o')\n",
    "\n",
    "        # Add labels and title\n",
    "        plt.title(\"Accuracy vs RMSE (Colored by PSS Value)\", fontsize=16)\n",
    "        plt.xlabel(\"Accuracy Level\", fontsize=14)\n",
    "        plt.ylabel(\"RMSE\", fontsize=14)\n",
    "\n",
    "        # Show the plot\n",
    "        plt.legend(title='PSS Value', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        file_name = '/PATH/RMSE_heatmaps/PSS_stimulus_progression_sub-' + participant_id + '_2.png'\n",
    "        plt.savefig(file_name)\n",
    "        #plt.show()\n",
    "\n",
    "    return pred_pss_value, pred_accuracy, rmse, rmse_table \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494afce6",
   "metadata": {},
   "source": [
    "REAL SAMPLE: Function to compute response counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285f4354",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_response_counts(participants_df, participant_id):\n",
    "    # Count the occurrences of each response code per Staircase_name\n",
    "    response_counts = participants_df.groupby(['Staircase_name', 'Response_code']).size().unstack(fill_value=0)\n",
    "    \n",
    "    # Rename columns for clarity\n",
    "    response_counts = response_counts.rename(columns={-1: \"Before_Response\", \n",
    "                                                       0: \"Same_Response\", \n",
    "                                                       1: \"After_Response\"})\n",
    "    \n",
    "    # Reset index for merging or visualization\n",
    "    response_counts = response_counts.reset_index()\n",
    "    \n",
    "    # Add participant_id column\n",
    "    response_counts['participant_id'] = participant_id\n",
    "    \n",
    "    return response_counts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb521d1e-94f6-49fc-96f4-35586ae2a8cd",
   "metadata": {},
   "source": [
    "REAL SAMPLE: Function to get p-value of rmse for allocated PSS, relative to the null, for each participant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342bc3e0-8f30-483b-851d-d7673bb53e84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_p_values(participant_data, df_vector_across_slices, plotting=False):\n",
    "    p_values = []\n",
    "    \n",
    "    for idx, participant in participant_data.iterrows():\n",
    "        participant_rmse = participant['RMSE']\n",
    "        participant_pss = participant['Pred_PSS_Value']\n",
    "        participant_accuracy = participant['Pred_Accuracy']\n",
    "        \n",
    "        # Extract the corresponding distribution from df_vector_across_slices\n",
    "        try:\n",
    "            null_vector = df_vector_across_slices.loc[participant_pss, participant_accuracy]\n",
    "            null_vector = [x for x in null_vector if not np.isnan(x)]  # Remove NaNs if any\n",
    "        except KeyError:\n",
    "            print(f\"Warning: PSS={participant_pss}, Accuracy={participant_accuracy} not found in df_vector_across_slices.\")\n",
    "            p_values.append(np.nan)\n",
    "            continue\n",
    "        \n",
    "        if len(null_vector) < 2:\n",
    "            #print(f\"Insufficient data for PSS={participant_pss}, Accuracy={participant_accuracy}.\")\n",
    "            # Assign p-value as 1/10,000 if there's no data (extremely unlikely)\n",
    "            print(f\"Extreme case: PSS={participant_pss}, Accuracy={participant_accuracy} Participant RMSE of {participant_rmse:.2f} is out of range of the null distribution.\")\n",
    "            p_values.append(1/10000)  # Reflect extreme unlikeliness\n",
    "            continue\n",
    "        \n",
    "        # Calculate mean and std deviation of the null distribution\n",
    "        mean_null = np.mean(null_vector)\n",
    "        std_null = np.std(null_vector, ddof=1)  # Sample std deviation\n",
    "        \n",
    "        # Calculate one-tailed p-value (Pr(X < participant_rmse))\n",
    "        p_value = norm.cdf(participant_rmse, loc=mean_null, scale=std_null)\n",
    "        \n",
    "        p_values.append(p_value)\n",
    "        \n",
    "        # Optional plotting\n",
    "        if plotting:\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            #sns.histplot(np.array(null_vector).ravel(), bins=50, kde=True, color='#F27B5A', label='Null Distribution', alpha=0.1)\n",
    "            sns.histplot(null_vector, bins=50, kde=False, color='#F27B5A', label='Null Distribution', alpha=0.1)\n",
    "            plt.axvline(participant_rmse, color='red', linestyle='--', linewidth=2, label=f'Participant RMSE ({participant_rmse:.2f})')\n",
    "            plt.title(f'Participant RMSE vs Null Distribution\\nPSS={participant_pss}, Accuracy={participant_accuracy}, p-value={p_value:.3f}')\n",
    "            plt.xlabel('RMSE')\n",
    "            plt.ylabel('Frequency')\n",
    "            plt.legend()\n",
    "            plt.close()  # Close the plot to suppress showing\n",
    "    \n",
    "    # Add p-values to the participant_data DataFrame\n",
    "    participant_data['p_value'] = p_values\n",
    "    return participant_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54f1ada",
   "metadata": {},
   "source": [
    "REAL SAMPLE: Function to calculate how many participants had >5 or <15 button presses per staircase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2458d737",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_participants_with_values_outside_range_for_staircase_type(pivoted_df, staircase_type='100'):\n",
    "    \"\"\"\n",
    "    This function counts the number of participants whose any of the response columns for the specified staircase type\n",
    "    (e.g., '100' or '400') fall outside the range [5, 15].\n",
    "\n",
    "    Parameters:\n",
    "    pivoted_df : DataFrame\n",
    "        The pivoted DataFrame with response counts.\n",
    "    staircase_type : str\n",
    "        The staircase type ('100' or '400') to filter for.\n",
    "\n",
    "    Returns:\n",
    "    int\n",
    "        The number of participants whose any of the response columns fall outside the range [5, 15].\n",
    "    \"\"\"\n",
    "    # Define the response columns\n",
    "    response_columns = ['Before_Response', 'Same_Response', 'After_Response']\n",
    "    \n",
    "    # Filter the columns based on the staircase type (e.g., '100' or '400')\n",
    "    staircase_columns = [f\"{col}_{staircase_type}\" for col in response_columns]\n",
    "    \n",
    "    # Check if any of the columns for this staircase type fall outside the range [5, 15]\n",
    "    condition = (pivoted_df[staircase_columns] <= 5) | (pivoted_df[staircase_columns] >= 15)\n",
    "    \n",
    "    # Get the participant IDs where any of the conditions hold true (i.e., any value falls outside [5, 15])\n",
    "    participants_outside_range = pivoted_df[condition.any(axis=1)].index.tolist()\n",
    "    \n",
    "    return participants_outside_range\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8baaed1c",
   "metadata": {},
   "source": [
    "REAL SAMPLE: Function to calculate how many participants had >5 or <15 button presses in either staircase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de26e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_overall_outside_range(pivoted_df):\n",
    "    \"\"\"\n",
    "    This function counts the number of unique participants whose any response column for either staircase type\n",
    "    ('100' or '400') falls outside the range [5, 15].\n",
    "\n",
    "    Parameters:\n",
    "    pivoted_df : DataFrame\n",
    "        The pivoted DataFrame with response counts.\n",
    "    \n",
    "    Returns:\n",
    "    unique_participants_outside_range : list\n",
    "        The list of unique participant IDs who meet the condition for either staircase type.\n",
    "    \"\"\"\n",
    "    # Get participants for staircase '100'\n",
    "    participants_100_outside_range = count_participants_with_values_outside_range_for_staircase_type(pivoted_df, staircase_type='100')\n",
    "\n",
    "    # Get participants for staircase '400'\n",
    "    participants_400_outside_range = count_participants_with_values_outside_range_for_staircase_type(pivoted_df, staircase_type='400')\n",
    "\n",
    "    # Combine the two lists to get unique participants (those who meet either or both conditions)\n",
    "    unique_participants_outside_range = list(set(participants_100_outside_range) | set(participants_400_outside_range))\n",
    "    \n",
    "    return unique_participants_outside_range"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca3ca7d",
   "metadata": {},
   "source": [
    "REAL SAMPLE: Filter to get unique participants who have Before_Response, Same_Response, or After_Response counts == either 0 or 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0956f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_participants_with_0_or_1(response_counts_df):\n",
    "    # Filter the rows where Before_Response, Same_Response, or After_Response counts are either 0 or 1\n",
    "    response_columns = [\"Before_Response\", \"Same_Response\", \"After_Response\"]\n",
    "    \n",
    "    # Create a mask for the condition: counts of 0 or 1 in any of the response columns\n",
    "    mask = response_counts_df[response_columns].isin([0, 1]).any(axis=1)\n",
    "    \n",
    "    # If we want to count the number of unique participants, we can filter on the 'participant_id' column\n",
    "    unique_participants = response_counts_df[mask]['participant_id'].nunique()\n",
    "    \n",
    "    return unique_participants"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f19cd7-5f34-488d-8c96-b653b60d6c0f",
   "metadata": {},
   "source": [
    "Define colour palettes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb77ad63-2e5d-4c99-8baf-c6b9fb12b301",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Referenece:\n",
    "null_colors = [\"#B5D5E0\", \"#4A738F\"]\n",
    "null_cmap = LinearSegmentedColormap.from_list(\"custom_heatmap\", null_colors, N=256)\n",
    "data = np.random.rand(10, 10)\n",
    "sns.heatmap(data, cmap=null_cmap)\n",
    "plt.show()\n",
    "\n",
    "#Null:\n",
    "null_colors = [\"#B1DFEE\", \"#5085E1\"]\n",
    "null_cmap = LinearSegmentedColormap.from_list(\"custom_heatmap\", null_colors, N=256)\n",
    "data = np.random.rand(10, 10)\n",
    "sns.heatmap(data, cmap=null_cmap)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#Simulated Participants:\n",
    "sim_participant_colors = [\"#EFD0B6\", \"#e86a36\"]\n",
    "sim_participant_cmap = LinearSegmentedColormap.from_list(\"custom_heatmap\", sim_participant_colors, N=256)\n",
    "data = np.random.rand(10, 10)\n",
    "sns.heatmap(data, cmap=sim_participant_cmap)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Real Participants:\n",
    "real_participant_colors = [\"#EEB0C4\", \"#e54e40\"]\n",
    "real_participant_cmap = LinearSegmentedColormap.from_list(\"custom_heatmap\", real_participant_colors, N=256)\n",
    "data = np.random.rand(10, 10)\n",
    "sns.heatmap(data, cmap=real_participant_cmap)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05b7dc0",
   "metadata": {},
   "source": [
    "# Demographics of real participants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c4d3c6-bfb3-459b-bf3e-b688d5793fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '/PATH/Redcap_data.csv'\n",
    "df = pd.read_csv(file_path)  # Skip the first row (header)\n",
    "#print(df)\n",
    "\n",
    "# Age summary (mean Â± std)\n",
    "age_mean = df['demo_age'].mean()\n",
    "age_std = df['demo_age'].std()\n",
    "\n",
    "print(f\"Mean Age: {age_mean:.2f} Â± {age_std:.2f}\")\n",
    "\n",
    "# Gender distribution (counts and percentages)\n",
    "gender_counts = df['demo_gender'].value_counts(dropna=False)\n",
    "gender_percent = df['demo_gender'].value_counts(normalize=True, dropna=False) * 100\n",
    "\n",
    "print(\"\\nGender distribution:\")\n",
    "for val in gender_counts.index:\n",
    "    label = f\"{val}\" if pd.notna(val) else \"Missing\"\n",
    "    print(f\"  {label}: {gender_counts[val]} ({gender_percent[val]:.2f}%)\")\n",
    "\n",
    "# Sex distribution (counts and percentages)\n",
    "sex_counts = df['demo_sex'].value_counts(dropna=False)\n",
    "sex_percent = df['demo_sex'].value_counts(normalize=True, dropna=False) * 100\n",
    "\n",
    "print(\"\\nSex distribution:\")\n",
    "for val in sex_counts.index:\n",
    "    label = f\"{val}\" if pd.notna(val) else \"Missing\"\n",
    "    print(f\"  {label}: {sex_counts[val]} ({sex_percent[val]:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d8ef43",
   "metadata": {},
   "source": [
    "# Run analysis:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0d80e7",
   "metadata": {},
   "source": [
    "[If all files have been generated click here](#skip-here)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a77a1a",
   "metadata": {},
   "source": [
    "REAL SAMPLE: \n",
    "\n",
    "Generate the participant trajectories\n",
    "Optional plotting\n",
    "Optional dropping of the first 5 trials\n",
    "\n",
    "For: PSS = 100-400; Accuracy = 0 - 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032169ce-d758-47de-b8e8-7227426598f1",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "pss_value_list = list(range(100, 401, 50))  # [50, 100, 150, ..., 600]\n",
    "accuracy_list = [round(i * 0.1, 1) for i in range(0, 11)]\n",
    "n_iterations = 100\n",
    "\n",
    "# Load simulation data\n",
    "simulation_df = pd.read_csv('/PATH/Reference_simulations.tsv', sep='\\t')\n",
    "\n",
    "# Clean column names (remove tabs, strip spaces)\n",
    "simulation_df.columns = [col.strip().replace('\\t', '') for col in simulation_df.columns]\n",
    "\n",
    "simulation_df['Staircase_name'] = simulation_df['Staircase_name'].astype(str)\n",
    "\n",
    "\n",
    "data_directory = '/PATH/SUBJECT_DATA/'\n",
    "\n",
    "for filename in tqdm(os.listdir(data_directory)):\n",
    "    if filename.endswith('.tsv'):  # Process only TSV files\n",
    "        # Extract participant ID: Keep only the numeric part (e.g., 'sub-005d' â†’ '005')\n",
    "        match = re.search(r'sub-(\\d+)', filename)\n",
    "        if not match:\n",
    "            continue  # Skip files that do not match the expected format\n",
    "        participant_id = match.group(1)  # Extract the numeric part (e.g., '005')\n",
    "        print(participant_id)\n",
    "        # Read the participant's data file\n",
    "        participants_df = pd.read_csv(os.path.join(data_directory, filename), sep='\\t',  skiprows=1)  \n",
    "        participants_df.columns = [col.strip().replace('\\t', '') for col in participants_df.columns]\n",
    "    \n",
    "        participants_df = participants_df[participants_df['Staircase_name'] != 'Training']\n",
    "        participants_df = participants_df[participants_df['Staircase_name'] != 'Post_task_question']\n",
    "        participants_df['Modified_Staircase_name'] = participants_df['Staircase_name'].str[:-2]\n",
    "        participants_df['Trial'] = participants_df['Trial'].astype(int)\n",
    "        \n",
    "        #plot_simulations_with_participant_data(simulation_df, participants_df, participant_id, pss_value_list, accuracy_list, n_iterations)\n",
    "        plot_participant_trajectory(participants_df, participant_id)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7051a6-4334-4570-85ec-d33a170d5345",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Data directory with real participant datafiles:\n",
    "data_directory = '/PATH/SUBJECT_DATA/'\n",
    "\n",
    "# Initialize DataFrames to store accumulated results\n",
    "final_pred_pss_accuracy = pd.DataFrame(columns=['Participant_ID', 'Pred_PSS_Value', 'Pred_Accuracy', 'RMSE'])\n",
    "all_mean_dfs = []\n",
    "\n",
    "# Define fixed ranges for the structure\n",
    "pred_pss_values = np.arange(100, 401, 50)  # Pred_PSS_Value: 0 to 400, increments of 50\n",
    "pred_accuracies = np.arange(0, 1.1, 0.1).round(1)  # Pred_Accuracy: 0.1 to 1.0, increments of 0.1\n",
    "pss_index = {v: i for i, v in enumerate(pred_pss_values)}\n",
    "accuracy_index = {v: i for i, v in enumerate(pred_accuracies)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7e51c6",
   "metadata": {},
   "source": [
    "# ðŸŽ² IDENTIFY RANDOM VS NON-RANDOM RESPONSES: Real Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fdff4c5-2f61-4e88-83c0-eb3f3544318b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "null_response_counts_df = pd.DataFrame()\n",
    "\n",
    "for i in tqdm(range(0, 100)):  \n",
    "    participants_file = f'/PATH/SIMULATED_NULL/10000_random_participant_simulations_{i}.tsv'\n",
    "    all_null_df = pd.read_csv(participants_file, sep='\\t')\n",
    "\n",
    "    # Get participant IDs\n",
    "    participant_ids = all_null_df['Participant_ID'].unique().tolist()\n",
    "        \n",
    "    # Loop over each participant ID\n",
    "    for participant_id in participant_ids[0:100]:\n",
    "        participants_df = all_null_df[all_null_df['Participant_ID'] == participant_id]\n",
    "        participants_df['Trial'] = participants_df['Trial'].astype(int)\n",
    "        participants_df['Modified_Staircase_name'] = participants_df['Staircase_name'].str[:-2]\n",
    "\n",
    "        # Recode trial numbers\n",
    "        participants_df.loc[:, 'Recode_Trial'] = np.where(\n",
    "            participants_df['Staircase_name'].str.contains('400'),\n",
    "            participants_df['Trial'], \n",
    "            participants_df['Trial'] + 15\n",
    "        )\n",
    "\n",
    "        # Calculate response counts per Staircase_name and Response_code\n",
    "        response_counts_df = compute_response_counts_ref(participants_df, participant_id, i)\n",
    "        null_response_counts_df = pd.concat([null_response_counts_df, response_counts_df], ignore_index=True)\n",
    "\n",
    "print(null_response_counts_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a07a90-3898-46ce-9574-3ec1700473f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "participant_response_counts_df = pd.DataFrame()\n",
    "\n",
    "for filename in tqdm(os.listdir(data_directory)):\n",
    "    if filename.endswith('.tsv'):  # Process only TSV files\n",
    "        # Extract participant ID: Keep only the numeric part (e.g., 'sub-005d' â†’ '005')\n",
    "        match = re.search(r'sub-(\\d+)', filename)\n",
    "        if not match:\n",
    "            continue  # Skip files that do not match the expected format\n",
    "        participant_id = match.group(1)  # Extract the numeric part (e.g., '005')\n",
    "\n",
    "        # Read the participant's data file\n",
    "        participants_df = pd.read_csv(os.path.join(data_directory, filename), sep='\\t', skiprows=1)\n",
    "        participants_df.columns = [col.strip().replace('\\t', '') for col in participants_df.columns]\n",
    "    \n",
    "        participants_df = participants_df[participants_df['Staircase_name'] != 'Training']\n",
    "        participants_df = participants_df[participants_df['Staircase_name'] != 'Post_task_question']\n",
    "        participants_df['Modified_Staircase_name'] = participants_df['Staircase_name'].str[:-2]\n",
    "        participants_df['Trial'] = participants_df['Trial'].astype(int)\n",
    "        \n",
    "        # Recode trial numbers\n",
    "        participants_df.loc[:, 'Recode_Trial'] = np.where(\n",
    "            participants_df['Staircase_name'].str.contains('400'),\n",
    "            participants_df['Trial'], \n",
    "            participants_df['Trial'] + 15\n",
    "        )\n",
    "        \n",
    "        # Calculate response counts per Staircase_name and Response_code\n",
    "        response_counts_df = compute_response_counts(participants_df, participant_id)\n",
    "        participant_response_counts_df = pd.concat([participant_response_counts_df, response_counts_df], ignore_index=True)\n",
    "\n",
    "print(participant_response_counts_df.head())\n",
    "participant_response_counts_df.to_csv('participant_response_counts_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5812c1c-a169-441f-a81a-cb1b6ac2c293",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create the 'Modified_Staircase_name' column if not done already\n",
    "participant_response_counts_df['Modified_Staircase_name'] = participant_response_counts_df['Staircase_name'].str[:-2]\n",
    "\n",
    "# Separate the data into two groups based on the 'Modified_Staircase_name' prefix\n",
    "df_100 = participant_response_counts_df[participant_response_counts_df['Modified_Staircase_name'].str.startswith('100')]\n",
    "df_400 = participant_response_counts_df[participant_response_counts_df['Modified_Staircase_name'].str.startswith('400')]\n",
    "\n",
    "# Group by participant_id and Modified_Staircase_name, and sum the responses\n",
    "summed_100 = df_100.groupby(['participant_id', 'Modified_Staircase_name'])[['Before_Response', 'Same_Response', 'After_Response']].sum()\n",
    "summed_400 = df_400.groupby(['participant_id', 'Modified_Staircase_name'])[['Before_Response', 'Same_Response', 'After_Response']].sum()\n",
    "\n",
    "# Now we concatenate the results for both 100 and 400 staircases\n",
    "combined_sums = pd.concat([summed_100, summed_400], axis=0)\n",
    "\n",
    "# Reset index to make 'participant_id' and 'Modified_Staircase_name' regular columns\n",
    "combined_sums.reset_index(inplace=True)\n",
    "\n",
    "# Pivot the table so that each participant has one row, and each staircase is a separate column\n",
    "pivoted_df = combined_sums.pivot_table(index='participant_id', \n",
    "                                       columns='Modified_Staircase_name', \n",
    "                                       values=['Before_Response', 'Same_Response', 'After_Response'], \n",
    "                                       aggfunc='sum')\n",
    "\n",
    "# Flatten the multi-index columns\n",
    "pivoted_df.columns = ['_'.join(col).strip() for col in pivoted_df.columns.values]\n",
    "#print(pivoted_df)\n",
    "\n",
    "\n",
    "\n",
    "# Create the 'Modified_Staircase_name' column for null_response_counts_df\n",
    "null_response_counts_df['Modified_Staircase_name'] = null_response_counts_df['Staircase_name'].str[:-2]\n",
    "\n",
    "# Separate the data into two groups based on the 'Modified_Staircase_name' prefix\n",
    "df_100_null = null_response_counts_df[null_response_counts_df['Modified_Staircase_name'].str.startswith('100')]\n",
    "df_400_null = null_response_counts_df[null_response_counts_df['Modified_Staircase_name'].str.startswith('400')]\n",
    "\n",
    "# Group by participant_id and Modified_Staircase_name, and sum the responses\n",
    "summed_100_null = df_100_null.groupby(['participant_id', 'Modified_Staircase_name'])[['Before_Response', 'Same_Response', 'After_Response']].sum()\n",
    "summed_400_null = df_400_null.groupby(['participant_id', 'Modified_Staircase_name'])[['Before_Response', 'Same_Response', 'After_Response']].sum()\n",
    "\n",
    "# Now we concatenate the results for both 100 and 400 staircases\n",
    "combined_sums_null = pd.concat([summed_100_null, summed_400_null], axis=0)\n",
    "combined_sums_null.reset_index(inplace=True)\n",
    "\n",
    "# Pivot the table so that each participant has one row, and each staircase is a separate column\n",
    "pivoted_null_df = combined_sums_null.pivot_table(index='participant_id', \n",
    "                                                 columns='Modified_Staircase_name', \n",
    "                                                 values=['Before_Response', 'Same_Response', 'After_Response'], \n",
    "                                                 aggfunc='sum')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "pivoted_null_df.columns = ['_'.join(col).strip() for col in pivoted_null_df.columns.values]\n",
    "print(pivoted_null_df.head())\n",
    "\n",
    "mean_null_row = pivoted_null_df.mean().to_frame().T\n",
    "mean_null_row.index = ['Null']  # Rename the index to 'Null'\n",
    "print(len(pivoted_null_df))\n",
    "# Now merge the mean row with the pivoted_df (participant's data)\n",
    "pivoted_df_with_null = pd.concat([pivoted_df, mean_null_row])\n",
    "\n",
    "# Show the resulting DataFrame\n",
    "print(pivoted_df_with_null)\n",
    "\n",
    "fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(12, 12))  # 3 rows, 2 columns\n",
    "axes = axes.flatten()\n",
    "numeric_columns = pivoted_df_with_null.select_dtypes(include=['number']).columns\n",
    "num_plots = min(len(numeric_columns), len(axes))\n",
    "for i in range(num_plots):\n",
    "    col = numeric_columns[i]\n",
    "    ax = axes[i]  # Get the corresponding subplot\n",
    "    sns.kdeplot(pivoted_df_with_null[col], ax=ax, fill=True, color='steelblue', alpha=0.6)\n",
    "    ax.set_title(f'KDE of {col}')\n",
    "    ax.set_xlabel('Value')\n",
    "    ax.set_ylabel('Frequency')\n",
    "    ax.set_xlim(0,30)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Filter the DataFrame for '100' responses\n",
    "pivoted_100_df = pivoted_df_with_null.filter(like='100', axis=1)\n",
    "\n",
    "real_colors = [ '#e0736f', '#e69fab','#dc5746']\n",
    "#fig, ax = plt.subplots(figsize=(20, 6), frameon=False)\n",
    "\n",
    "# Plot for all '100' responses\n",
    "pivoted_100_df.plot(kind='bar', stacked=True, figsize=(20, 6),color=real_colors)\n",
    "plt.xlabel(\"Participant\")\n",
    "plt.ylabel(\"Sum of Responses\")\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "plt.title(\"Summed '100' Response Percentages by Participant\")\n",
    "plt.legend(title=\"Response Type\", loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"100_Responses.png\", bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Filter the DataFrame for '400' responses\n",
    "pivoted_400_df = pivoted_df_with_null.filter(like='400', axis=1)\n",
    "\n",
    "# Plot for all '400' responses\n",
    "pivoted_400_df.plot(kind='bar', stacked=True, figsize=(20, 6), color=real_colors)\n",
    "plt.xlabel(\"Participant\")\n",
    "plt.ylabel(\"Sum of Responses\")\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "plt.title(\"Summed '400' Response Percentages by Participant\")\n",
    "plt.legend(title=\"Response Type\", loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"400_Responses.png\", bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Get participants whose any response values for staircase '100' fall outside the range [5, 15]\n",
    "participants_100_outside_range = count_participants_with_values_outside_range_for_staircase_type(pivoted_df_with_null, staircase_type='100')\n",
    "print(f\"Participants with any response values outside the range [5, 15] for staircase '100': {len(participants_100_outside_range)}\")\n",
    "\n",
    "# Get participants whose any response values for staircase '400' fall outside the range [5, 15]\n",
    "participants_400_outside_range = count_participants_with_values_outside_range_for_staircase_type(pivoted_df_with_null, staircase_type='400')\n",
    "print(f\"Participants with any response values == or outside the range [5, 15] for staircase '400': {len(participants_400_outside_range)}\")\n",
    "\n",
    "# Get all unique participants whose any response values fall outside the range [5, 15] for either staircase type\n",
    "overall_outside_range_participants = count_overall_outside_range(pivoted_df_with_null)\n",
    "print(f\"Overall unique participants with any response values == or outside the range [5, 15] (for both '100' or '400'): {len(overall_outside_range_participants)}\")\n",
    "\n",
    "perc = (len(overall_outside_range_participants)/ len(pivoted_df))\n",
    "print(perc)\n",
    "\n",
    "# Optionally print the actual IDs of participants\n",
    "print(\"Keep - Unique participants who meet the condition (either or both staircase types):\")\n",
    "print(overall_outside_range_participants)\n",
    "print(len(overall_outside_range_participants))\n",
    "\n",
    "\n",
    "\n",
    "#participant_response_counts_df\n",
    "all_ids = set(participant_response_counts_df['participant_id'].astype(str))\n",
    "keep_ids = set(overall_outside_range_participants)\n",
    "\n",
    "# Get those NOT in overall_outside_range_participants\n",
    "exclude_ids_random = list(all_ids - keep_ids)\n",
    "print('Exclude: ', exclude_ids_random)\n",
    "print(len(exclude_ids_random))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808dbff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load all the simulated participants to plot them all together\n",
    "\n",
    "sim_participant_response_counts_df = pd.read_csv('sim_participant_response_counts_df.csv')\n",
    "sim_participant_response_counts_df['Modified_Staircase_name'] = sim_participant_response_counts_df['Staircase_name'].str[:-2]\n",
    "#print(sim_participant_response_counts_df)\n",
    "\n",
    "# Separate the data into two groups based on the 'Modified_Staircase_name' prefix\n",
    "df_100 = sim_participant_response_counts_df[sim_participant_response_counts_df['Modified_Staircase_name'].str.startswith('100')]\n",
    "df_400 = sim_participant_response_counts_df[sim_participant_response_counts_df['Modified_Staircase_name'].str.startswith('400')]\n",
    "#print(df_100)\n",
    "\n",
    "# Group by participant_id and Modified_Staircase_name, and sum the responses\n",
    "summed_100 = df_100.groupby(['participant_id', 'Modified_Staircase_name'])[['Before_Response', 'Same_Response', 'After_Response']].sum()\n",
    "summed_400 = df_400.groupby(['participant_id', 'Modified_Staircase_name'])[['Before_Response', 'Same_Response', 'After_Response']].sum()\n",
    "\n",
    "# Now we concatenate the results for both 100 and 400 staircases\n",
    "combined_sums = pd.concat([summed_100, summed_400], axis=0)\n",
    "\n",
    "# Reset index to make 'participant_id' and 'Modified_Staircase_name' regular columns\n",
    "combined_sums.reset_index(inplace=True)\n",
    "\n",
    "# Pivot the table so that each participant has one row, and each staircase is a separate column\n",
    "pivoted_df_sim = combined_sums.pivot_table(index='participant_id', \n",
    "                                       columns='Modified_Staircase_name', \n",
    "                                       values=['Before_Response', 'Same_Response', 'After_Response'], \n",
    "                                       aggfunc='sum')\n",
    "\n",
    "# Flatten the multi-index columns\n",
    "pivoted_df_sim.columns = ['_'.join(col).strip() for col in pivoted_df_sim.columns.values]\n",
    "#print(pivoted_df)\n",
    "\n",
    "\n",
    "##COMBINED PLOT:\n",
    "# Set font size to 7 points and update figure size (5cm wide = 1.97 inches)\n",
    "plt.rcParams.update({'font.size': 7})\n",
    "\n",
    "# Set plot dimensions: 5cm wide (~1.97 in), height to preserve aspect (e.g., 3 in)\n",
    "fig_width_cm = 6\n",
    "fig_height_cm = 7.5  # adjust as needed\n",
    "fig_size_in = (fig_width_cm / 2.54, fig_height_cm / 2.54)  # convert cm to inches\n",
    "\n",
    "null_colors = ['#87afe6', '#b1d8ec', '#6991e1']\n",
    "sim_colors = ['#e49971', '#ecd1b7', '#e17a4b']\n",
    "real_colors = [ '#e0736f', '#e69fab','#dc5746']\n",
    "\n",
    "def get_cols_by_response_and_staircase(df, response_type, staircase_prefix):\n",
    "    # Return list of columns matching exact pattern ResponseType_StaircasePrefix\n",
    "    return [col for col in df.columns if col == f\"{response_type}_{staircase_prefix}\"]\n",
    "\n",
    "# Plot for 100 staircases\n",
    "plt.figure(figsize=fig_size_in, dpi = 300)\n",
    "for i, response in enumerate(['Before_Response', 'Same_Response', 'After_Response']):\n",
    "    cols = get_cols_by_response_and_staircase(pivoted_null_df, response, '100')\n",
    "    if len(cols) == 0:\n",
    "        print(f\"No columns found for {response} 100\")\n",
    "        continue\n",
    "    # cols is a list with one element (the exact column name)\n",
    "    sns.kdeplot(pivoted_null_df[cols[0]], fill=True, alpha=0.3, color=null_colors[i], label=f'Null 100 - {response.split(\"_\")[0]}')\n",
    "    sns.kdeplot(pivoted_df_sim[cols[0]], fill=True, alpha=0.3, color=sim_colors[i], label=f'Sim 100 - {response.split(\"_\")[0]}')\n",
    "    sns.kdeplot(pivoted_df[cols[0]], fill=True, alpha=0.3, color=real_colors[i], label=f'Real 100 - {response.split(\"_\")[0]}')\n",
    "plt.title(\"KDE of '100' Staircases Responses\")\n",
    "plt.xlabel('Number of button presses')\n",
    "plt.ylabel('Density')\n",
    "plt.xlim(0, 30)\n",
    "#plt.legend()\n",
    "sns.despine()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot for 400 staircases\n",
    "plt.figure(figsize=fig_size_in, dpi = 300)\n",
    "for i, response in enumerate(['Before_Response', 'Same_Response', 'After_Response']):\n",
    "    cols = get_cols_by_response_and_staircase(pivoted_null_df, response, '400')\n",
    "    if len(cols) == 0:\n",
    "        print(f\"No columns found for {response} 400\")\n",
    "        continue\n",
    "    # cols is a list with one element (the exact column name)\n",
    "    sns.kdeplot(pivoted_null_df[cols[0]], fill=True, alpha=0.3, color=null_colors[i], label=f'Null 400 - {response.split(\"_\")[0]}')\n",
    "    sns.kdeplot(pivoted_df_sim[cols[0]], fill=True, alpha=0.3, color=sim_colors[i], label=f'Sim 400 - {response.split(\"_\")[0]}')\n",
    "    sns.kdeplot(pivoted_df[cols[0]], fill=True, alpha=0.3, color=real_colors[i], label=f'Real 400 - {response.split(\"_\")[0]}')\n",
    "plt.title(\"KDE of '400' Staircases Responses\")\n",
    "plt.xlabel('Number of button presses')\n",
    "plt.ylabel('Density')\n",
    "plt.xlim(0, 30)\n",
    "#plt.legend()\n",
    "sns.despine()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047df777-39bb-4eab-84f6-cf98b46f8cba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define response types\n",
    "response_types = [\"Before_Response\", \"Same_Response\", \"After_Response\"]\n",
    "\n",
    "# Extract unique participant IDs\n",
    "participant_ids = participant_response_counts_df['participant_id'].unique()\n",
    "\n",
    "# Create a color mapping\n",
    "color_mapping = {}\n",
    "colors = plt.cm.tab20.colors  # Get the tab20 colormap\n",
    "for i, participant in enumerate(participant_ids):\n",
    "    if participant == \"Null\":\n",
    "        color_mapping[participant] = \"gray\"  # Assign gray to \"Null\"\n",
    "    else:\n",
    "        color_mapping[participant] = colors[i % len(colors)]  # Assign colors from tab20 cyclically\n",
    "\n",
    "# Plot for each response type\n",
    "for response in response_types:\n",
    "    plt.figure(figsize=(20, 5))\n",
    "\n",
    "    # Group data by participant_id and Staircase_name\n",
    "    response_data = participant_response_counts_df[['participant_id', 'Staircase_name', response]]\n",
    "    \n",
    "    # Pivot the data\n",
    "    pivot_data = response_data.pivot_table(index='participant_id', columns='Staircase_name', values=response, aggfunc='mean')\n",
    "\n",
    "    # Create a list of colors for each bar\n",
    "    bar_colors = []\n",
    "    for participant in pivot_data.index:\n",
    "        bar_colors.extend([color_mapping[participant]] * len(pivot_data.columns))\n",
    "\n",
    "    # Plot the bars\n",
    "    ax = pivot_data.plot(\n",
    "        kind='bar',\n",
    "        stacked=False,\n",
    "        color=bar_colors,\n",
    "        figsize=(20, 5),\n",
    "        legend=True\n",
    "    )\n",
    "\n",
    "    # Add a horizontal line at y = 1\n",
    "    ax.axhline(y=1, color='black', linestyle='--', linewidth=1)\n",
    "\n",
    "    # Customize the plot\n",
    "    plt.xlabel(\"Participant ID\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.title(f\"{response} Responses\")\n",
    "    plt.xticks(rotation=45, fontsize=8)\n",
    "    plt.legend(title=\"Staircase Name\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a2cfe6-0e42-44e3-98ce-0354e00bd084",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Count participants in the null_response_counts_df\n",
    "null_count = count_participants_with_0_or_1(null_response_counts_df)\n",
    "\n",
    "# Count participants in the participant_response_counts_df\n",
    "participant_count = count_participants_with_0_or_1(participant_response_counts_df)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Number of unique participants with 0 or 1 response count in 'null_response_counts_df': {null_count}\")\n",
    "print(f\"Number of unique participants with 0 or 1 response count in 'participant_response_counts_df': {participant_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2300715a-e430-4663-baed-542be81b4958",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "testme_participant_df = pd.DataFrame()\n",
    "\n",
    "for filename in os.listdir(data_directory):\n",
    "    if filename.endswith('.tsv'):  # Process only TSV files\n",
    "        # Extract participant ID: Keep only the numeric part (e.g., 'sub-005d' â†’ '005')\n",
    "        match = re.search(r'sub-(\\d+)', filename)\n",
    "        if not match:\n",
    "            continue  # Skip files that do not match the expected format\n",
    "        participant_id = match.group(1)  # Extract the numeric part (e.g., '005')\n",
    "        #print(participant_id)\n",
    "        # Read the participant's data file\n",
    "        participants_df = pd.read_csv(os.path.join(data_directory, filename), sep='\\t',  skiprows=1)  \n",
    "        participants_df.columns = [col.strip().replace('\\t', '') for col in participants_df.columns]\n",
    "    \n",
    "        participants_df = participants_df[participants_df['Staircase_name'] != 'Training']\n",
    "        participants_df = participants_df[participants_df['Staircase_name'] != 'Post_task_question']\n",
    "        participants_df['Participant_ID'] = participant_id\n",
    "        # Clean column names (remove tabs, strip spaces)\n",
    "        \n",
    "        #print(participants_df)\n",
    "        \n",
    "        participants_df['Trial'] = participants_df['Trial'].astype(int)\n",
    "        participants_df['Modified_Staircase_name'] = participants_df['Staircase_name'].str[:-2]\n",
    "\n",
    "        # Recode trial numbers\n",
    "        participants_df.loc[:, 'Recode_Trial'] = np.where(\n",
    "            participants_df['Staircase_name'].str.contains('400'),\n",
    "            participants_df['Trial'], \n",
    "            participants_df['Trial'] + 15\n",
    "        )\n",
    "        \n",
    "        testme_participant_df = pd.concat([testme_participant_df, participants_df], ignore_index=True)\n",
    "        \n",
    "        \n",
    "print(testme_participant_df)\n",
    "testme_participant_df.to_csv('trial_by_trial_button_press.tsv', sep = '\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54918f65-35b4-4a5c-9522-72bdfeebd835",
   "metadata": {},
   "source": [
    "# ðŸ§  IDENTIFY COGNITIVE STRATEGY: Real Sample\n",
    "\n",
    "Check if trajectories are parallel; diverging; converging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762b9d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "null_diff_df = pd.DataFrame()\n",
    "\n",
    "for i in tqdm(range(0,100)):  \n",
    "    participants_file = f'/PATH/SIMULATED_NULL/10000_random_participant_simulations_{i}.tsv'\n",
    "    all_null_df = pd.read_csv(participants_file, sep='\\t')\n",
    "\n",
    "    # Get participant IDs\n",
    "    participant_ids = []\n",
    "    participant_ids = all_null_df['Participant_ID'].unique().tolist()\n",
    "    #print(participant_ids)\n",
    "        \n",
    "    # Loop over each participant ID\n",
    "    for participant_id in participant_ids[0:100]:\n",
    "        participants_df = all_null_df[all_null_df['Participant_ID'] == participant_id]\n",
    "        #print(participants_df)\n",
    "        participants_df = participants_df.copy()\n",
    "        participants_df.loc[:, 'Trial'] = participants_df['Trial'].astype(int)\n",
    "        participants_df.loc[:, 'Modified_Staircase_name'] = participants_df['Staircase_name'].str[:-2]\n",
    "        participants_df.loc[:, 'Recode_Trial'] = np.where(\n",
    "            participants_df['Staircase_name'].str.contains('400'),\n",
    "            participants_df['Trial'], \n",
    "            participants_df['Trial'] + 15\n",
    "        )\n",
    "\n",
    "        #print(participants_df)\n",
    "\n",
    "         # For 400ms (trials 1-15)\n",
    "        mean_delays_400 = participants_df[participants_df['Staircase_name'].str.contains('400')] \\\n",
    "                            .groupby('Recode_Trial')['Current Delay'].mean()\n",
    "\n",
    "        # For 100ms (trials 16-30)\n",
    "        mean_delays_100 = participants_df[participants_df['Staircase_name'].str.contains('100')] \\\n",
    "                            .groupby('Recode_Trial')['Current Delay'].mean()\n",
    "\n",
    "        # Combine the two mean delay series (400_1, 400_2, and 100_1, 100_2) into one DataFrame\n",
    "        mean_delays = pd.concat([mean_delays_400, mean_delays_100])\n",
    "\n",
    "        # Reindex the trials: 1-15 for 400ms, 16-30 for 100ms\n",
    "        mean_delays.index = np.arange(1, 31)\n",
    "\n",
    "        start_diff = mean_delays[1] - mean_delays[16]\n",
    "        end_diff = mean_delays[15] - mean_delays[30]\n",
    "\n",
    "        new_row = pd.DataFrame([{\n",
    "        'Participant_ID': participant_id,\n",
    "        'start_400': mean_delays[1],\n",
    "        'end_400':  mean_delays[15],\n",
    "        'start_100':  mean_delays[16],\n",
    "        'end_100': mean_delays[30],\n",
    "        'start_diff': start_diff,\n",
    "        'end_diff': end_diff\n",
    "        }])\n",
    "\n",
    "        null_diff_df = pd.concat([null_diff_df, new_row], ignore_index=True)\n",
    "\n",
    "#Calculate: The distance moved (-ve = converging; +ve = diverging)\n",
    "null_diff_df['movement'] = null_diff_df['end_diff'] - null_diff_df['start_diff'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fceb5470-5312-47e4-ba1b-422a75944734",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "real_participant_diff_df = pd.DataFrame()\n",
    "\n",
    "for filename in os.listdir(data_directory):\n",
    "    if filename.endswith('.tsv'):  # Process only TSV files\n",
    "        # Extract participant ID: Keep only the numeric part (e.g., 'sub-005d' â†’ '005')\n",
    "        match = re.search(r'sub-(\\d+)', filename)\n",
    "        if not match:\n",
    "            continue  # Skip files that do not match the expected format\n",
    "        participant_id = match.group(1)  # Extract the numeric part (e.g., '005')\n",
    "        #print(participant_id)\n",
    "        # Read the participant's data file\n",
    "        participants_df = pd.read_csv(os.path.join(data_directory, filename), sep='\\t',  skiprows=1)  \n",
    "        participants_df.columns = [col.strip().replace('\\t', '') for col in participants_df.columns]\n",
    "    \n",
    "        participants_df = participants_df[participants_df['Staircase_name'] != 'Training']\n",
    "        participants_df = participants_df[participants_df['Staircase_name'] != 'Post_task_question']\n",
    "        # Clean column names (remove tabs, strip spaces)\n",
    "        \n",
    "        #print(participants_df)\n",
    "        \n",
    "        participants_df['Trial'] = participants_df['Trial'].astype(int)\n",
    "        participants_df['Modified_Staircase_name'] = participants_df['Staircase_name'].str[:-2]\n",
    "\n",
    "        # Recode trial numbers\n",
    "        participants_df.loc[:, 'Recode_Trial'] = np.where(\n",
    "            participants_df['Staircase_name'].str.contains('400'),\n",
    "            participants_df['Trial'], \n",
    "            participants_df['Trial'] + 15\n",
    "        )\n",
    "         # For 400ms (trials 1-15)\n",
    "        mean_delays_400 = participants_df[participants_df['Staircase_name'].str.contains('400')] \\\n",
    "                            .groupby('Recode_Trial')['Current Delay'].mean()\n",
    "\n",
    "        # For 100ms (trials 16-30)\n",
    "        mean_delays_100 = participants_df[participants_df['Staircase_name'].str.contains('100')] \\\n",
    "                            .groupby('Recode_Trial')['Current Delay'].mean()\n",
    "\n",
    "        # Combine the two mean delay series (400_1, 400_2, and 100_1, 100_2) into one DataFrame\n",
    "        mean_delays = pd.concat([mean_delays_400, mean_delays_100])\n",
    "        \n",
    "        # Reindex the trials: 1-15 for 400ms, 16-30 for 100ms\n",
    "        mean_delays.index = np.arange(1, 31)\n",
    "\n",
    "        start_diff = mean_delays[1] - mean_delays[16]\n",
    "        end_diff = mean_delays[15] - mean_delays[30]\n",
    "            \n",
    "        new_row = pd.DataFrame({\n",
    "            'Participant_ID': [participant_id],\n",
    "            'start_400': [mean_delays[1]],\n",
    "            'end_400': [mean_delays[15]],\n",
    "            'start_100': [mean_delays[16]],\n",
    "            'end_100': [mean_delays[30]],\n",
    "            'start_diff': [start_diff],\n",
    "            'end_diff': [end_diff]\n",
    "                })\n",
    "\n",
    "        # Use pd.concat to append the new row to the existing DataFrame\n",
    "        real_participant_diff_df = pd.concat([real_participant_diff_df, new_row], ignore_index=True)\n",
    "\n",
    "display(HTML(real_participant_diff_df.head().to_html()))\n",
    "\n",
    "#âš ï¸ IF EXCLUDING PARTICIPANTS WHO HAVE RANDOM BUTTON PRESS: RUN THIS LINE\n",
    "real_participant_diff_df = real_participant_diff_df[\n",
    "    real_participant_diff_df['Participant_ID'].isin(overall_outside_range_participants)\n",
    "]\n",
    "\n",
    "print(len(real_participant_diff_df))\n",
    "\n",
    "\n",
    "#Calculate: The distance moved (-ve = converging; +ve = diverging)\n",
    "real_participant_diff_df['movement'] = real_participant_diff_df['end_diff'] - real_participant_diff_df['start_diff'] \n",
    "\n",
    "# Parallel: End_diff = Start_diff = 300ms\n",
    "real_participant_parallel_movers = real_participant_diff_df[((real_participant_diff_df[\"end_diff\"] == 300) & (real_participant_diff_df[\"end_400\"] != 400))]\n",
    "\n",
    "# Divergent: End_diff > Start_diff \n",
    "real_participant_divergent_movers = real_participant_diff_df[(real_participant_diff_df[\"end_diff\"] >= 301)]\n",
    "\n",
    "# Convergent: End_diff < Start_diff \n",
    "real_participant_converge_movers = real_participant_diff_df[(real_participant_diff_df[\"end_diff\"] < 300)]\n",
    "\n",
    "real_participant_diff_df.to_csv(\"Real_participant_diff_df_100_400.csv\", index=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdea503b-47e9-42d8-864b-e536baf29a24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Summary:\n",
    "print(len(real_participant_parallel_movers),':', len(real_participant_parallel_movers)/42*100,'%: Parallel lines')\n",
    "print(len(real_participant_divergent_movers),':', len(real_participant_divergent_movers)/42*100,'%: Have directions away from converging i.e. acc less than 0.3')\n",
    "print(len(real_participant_converge_movers),':', len(real_participant_converge_movers)/42*100,'%: Have directions towards converging i.e. acc > than 0.3')\n",
    "print('\\n\\nMean end delay difference of null: ', real_participant_diff_df['end_diff'].mean())\n",
    "print('STD end delay difference of null: ', real_participant_diff_df['end_diff'].std())\n",
    "\n",
    "#Parallel:\n",
    "print('\\n\\nMean end delay of 400 staircase of parallel movers: ', real_participant_parallel_movers['end_400'].mean())\n",
    "print('Mean end delay 100 staircase of parallel movers: ',real_participant_parallel_movers['end_100'].mean())\n",
    "\n",
    "#Divergent:\n",
    "print('\\n\\nMean change from 300ms diff of divergent movers: ',real_participant_divergent_movers['movement'].mean())\n",
    "print('STD change from 300ms diff of divergent movers: ',real_participant_divergent_movers['movement'].std())\n",
    "print('Median end delay difference of divergent movers: ', real_participant_divergent_movers['end_diff'].median())\n",
    "print('Min end delay difference of divergent movers: ',real_participant_divergent_movers['end_diff'].min())\n",
    "print('Max end delay difference of divergent movers: ',real_participant_divergent_movers['end_diff'].max())\n",
    "\n",
    "#Convergent:\n",
    "print('\\n\\nMean change from 300ms diff of convergent movers: ', real_participant_converge_movers['movement'].mean())\n",
    "print('STF change from 300ms diff of convergent movers: ', real_participant_converge_movers['movement'].std())\n",
    "print('Median end delay difference of convergent movers: ', real_participant_converge_movers['end_diff'].median())\n",
    "print('Mean end delay difference of convergent movers: ',real_participant_converge_movers['end_diff'].mean())\n",
    "print('STD end delay difference of convergent movers: ',real_participant_converge_movers['end_diff'].std())\n",
    "print('Min end delay difference of convergent movers: ',real_participant_converge_movers['end_diff'].min())\n",
    "print('Max end delay difference of convergent movers: ',real_participant_converge_movers['end_diff'].max())\n",
    "count = (real_participant_diff_df['end_diff'] <= 0).sum()\n",
    "print('Number of convergent movers who converge completely:',count)\n",
    "\n",
    "#REFERENCE FILES:\n",
    "ref_diff_df = pd.read_csv(\"/PATH/Reference_diff_df_100_400.csv\")  \n",
    "\n",
    "# Plot histogram\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.histplot(ref_diff_df['end_diff'], bins=150, kde=False, color=\"#4A738F\")\n",
    "#sns.histplot(null_diff_df['end_diff'], bins=150, kde=True, color=\"#5085E1\")\n",
    "sns.histplot(real_participant_diff_df['end_diff'], bins=150, kde=False, color=\"#e54e40\")\n",
    "plt.xlabel(\"End Difference (ms)\", fontsize=12)\n",
    "plt.ylabel(\"Frequency\", fontsize=12)\n",
    "plt.title(\"End difference in \\n Reference and Real Participants\", fontsize=14)\n",
    "plt.show()\n",
    "\n",
    "# Scatter plot\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.axhspan(299, 301, color='grey', alpha=0.3)\n",
    "plt.axhspan(301, 600, color='lightgrey', alpha=0.3)\n",
    "scatter = sns.scatterplot(\n",
    "    x=ref_diff_df['PSS_Value'],  \n",
    "    y=ref_diff_df[\"end_diff\"],\n",
    "    hue=ref_diff_df[\"Accuracy\"],\n",
    "    palette=\"viridis\",\n",
    "    sizes=(20, 200),  \n",
    "    edgecolor=\"black\"\n",
    ")\n",
    "scatter = sns.scatterplot(\n",
    "    x=real_participant_diff_df.index,  \n",
    "    y=real_participant_diff_df[\"end_diff\"],\n",
    "    #hue=real_participant_diff_df[\"Participant_ID\"],\n",
    "    color=\"#e54e40\",\n",
    "    sizes=(20, 200),  \n",
    "    edgecolor=\"black\",\n",
    "    alpha = 0.5\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Participants / PSS\")\n",
    "plt.ylabel(\"End Difference (end_diff)\")\n",
    "plt.title(\"Scatter Plot of end_diff vs Index\")\n",
    "\n",
    "plt.legend(title=\"Accuracy / Participant\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "plt.show()\n",
    "\n",
    "exclude_ids_parallel = set(real_participant_parallel_movers['Participant_ID'].astype(str))\n",
    "print('Exclude: ', exclude_ids_parallel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6584fb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_participant_diff_df = pd.read_csv(\"10000_simulated_participants_diff_df_100_400.csv\")\n",
    "null_diff_df = pd.read_csv(\"Null_diff_df_100_400.csv\")  \n",
    "real_participant_diff_df = pd.read_csv(\"Real_participant_diff_df_100_400.csv\")  \n",
    "\n",
    "\n",
    "# Colors (third from each palette)\n",
    "null_color = null_colors[2]   # '#6991e1'\n",
    "sim_color = sim_colors[2]     # '#e17a4b'\n",
    "real_color = real_colors[2] # '#dc5746'\n",
    "\n",
    "# Plot KDE for 'end_diff' for each dataset\n",
    "fig_width_cm = 6\n",
    "fig_height_cm = 7.8  # adjust as needed\n",
    "fig_size_in = (fig_width_cm / 2.54, fig_height_cm / 2.54)  # convert cm to inches\n",
    "\n",
    "plt.figure(figsize=fig_size_in, dpi=300)\n",
    "\n",
    "# KDE plots\n",
    "sns.kdeplot(null_diff_df[\"end_diff\"], fill=True, alpha=0.3, color=null_color, label=\"Null - end_diff\")\n",
    "sns.kdeplot(sim_participant_diff_df[\"end_diff\"], fill=True, alpha=0.3, color=sim_color, label=\"Simulated - end_diff\")\n",
    "sns.kdeplot(real_participant_diff_df[\"end_diff\"], fill=True, alpha=0.3, color=real_color, label=\"Real - end_diff\")\n",
    "\n",
    "# Styling\n",
    "plt.title(\"Distribution of 'end_diff' Across Datasets\")\n",
    "plt.xlabel(\"Difference in mean delay value of \\n100ms and 400ms staircase at trial 15\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.xlim(0, 600)  # Adjust based on your data range\n",
    "sns.despine()\n",
    "#plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91315553",
   "metadata": {},
   "source": [
    "# â¤ï¸ASSIGN PSS AND ACCURACY VALUES: Real Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55672f4d-abec-415d-8cad-04ae91d81690",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#âš ï¸ IF EXCLUDING PARTICIPANTS WHO HAVE RANDOM BUTTON PRESS AND PARALLEL LINES\n",
    "excluded_ids = set(exclude_ids_parallel) | set(exclude_ids_random)\n",
    "\n",
    "\n",
    "# Load simulation data\n",
    "simulation_df = pd.read_csv('/PATH/Reference_simulations.tsv', sep='\\t')\n",
    "simulation_df['Staircase_name'] = simulation_df['Staircase_name'].astype(str)\n",
    "pivot_df = reference_matrix(simulation_df, pred_pss_values, pred_accuracies, drop = True, plots=False)\n",
    "\n",
    "all_mean_dfs = pd.DataFrame()\n",
    "final_pred_pss_accuracy = pd.DataFrame(columns=['Participant_ID', 'Pred_PSS_Value', 'Pred_Accuracy', 'RMSE'])\n",
    "\n",
    "# Loop over each file in the directory\n",
    "for filename in os.listdir(data_directory):\n",
    "    if filename.endswith('.tsv'):\n",
    "        match = re.search(r'sub-(\\d+)', filename)\n",
    "        if not match:\n",
    "            continue\n",
    "\n",
    "        participant_id = match.group(1)\n",
    "\n",
    "        # âš ï¸ Exclude if participant_id is in either exclusion set\n",
    "        if participant_id in excluded_ids:\n",
    "            print(f\"Skipping excluded participant: {participant_id}\")\n",
    "            continue\n",
    "        \n",
    "        #print(participant_id)\n",
    "        # Read the participant's data file\n",
    "        participants_df = pd.read_csv(os.path.join(data_directory, filename), sep='\\t',  skiprows=1)  \n",
    "        participants_df.columns = [col.strip().replace('\\t', '') for col in participants_df.columns]\n",
    "    \n",
    "        participants_df = participants_df[participants_df['Staircase_name'] != 'Training']\n",
    "        participants_df = participants_df[participants_df['Staircase_name'] != 'Post_task_question']\n",
    "        # Clean column names (remove tabs, strip spaces)\n",
    "        \n",
    "        #print(participants_df)\n",
    "        \n",
    "        participants_df['Trial'] = participants_df['Trial'].astype(int)\n",
    "        participants_df['Modified_Staircase_name'] = participants_df['Staircase_name'].str[:-2]\n",
    "\n",
    "        # Recode trial numbers\n",
    "        participants_df.loc[:, 'Recode_Trial'] = np.where(\n",
    "            participants_df['Staircase_name'].str.contains('400'),\n",
    "            participants_df['Trial'], \n",
    "            participants_df['Trial'] + 15\n",
    "        )\n",
    "        \n",
    "        # Temporary storage for current iteration's results\n",
    "        Pred_PSS_Accuracy = pd.DataFrame(columns=['Participant_ID', 'Pred_PSS_Value', 'Pred_Accuracy', 'RMSE'])\n",
    "        \n",
    "        # Process data for the current participant ID\n",
    "        pred_pss_value, pred_accuracy, rmse, rmse_table = participant_matrix(\n",
    "            participants_df, participant_id, pivot_df, drop=True, plots=False, verbose=False\n",
    "        )\n",
    "        \n",
    "        # Append results to Pred_PSS_Accuracy\n",
    "        new_row = pd.DataFrame({\n",
    "            'Participant_ID': [participant_id],\n",
    "            'Pred_PSS_Value': [pred_pss_value],\n",
    "            'Pred_Accuracy': [pred_accuracy],\n",
    "            'RMSE': [rmse]\n",
    "        })\n",
    "\n",
    "        # Use pd.concat to append the new row to the existing DataFrame\n",
    "        Pred_PSS_Accuracy = pd.concat([Pred_PSS_Accuracy, new_row], ignore_index=True)\n",
    "                \n",
    "        # Create group structure for aligning means\n",
    "        group_structure = pd.DataFrame(\n",
    "            [(v, a) for v in pred_pss_values for a in pred_accuracies],\n",
    "            columns=['Pred_PSS_Value', 'Pred_Accuracy']\n",
    "        )\n",
    "        \n",
    "        # Group by and calculate means\n",
    "        grouped_rmse_mean = Pred_PSS_Accuracy.groupby([\"Pred_PSS_Value\", \"Pred_Accuracy\"])[\"RMSE\"].mean().reset_index()\n",
    "        file_mean = group_structure.merge(grouped_rmse_mean, on=['Pred_PSS_Value', 'Pred_Accuracy'], how='left')\n",
    "\n",
    "        # Pivot to create mean_df\n",
    "        mean_df = file_mean.pivot(index=\"Pred_PSS_Value\", columns=\"Pred_Accuracy\", values=\"RMSE\")\n",
    "\n",
    "        # Convert mean_df to a slice of the 3D matrix\n",
    "        slice_matrix = np.full((len(pred_pss_values), len(pred_accuracies)), np.nan)\n",
    "        for pss_val, row in mean_df.iterrows():\n",
    "            for acc_val, rmse in row.items():\n",
    "                if not pd.isna(rmse):\n",
    "                    slice_matrix[pss_index[pss_val], accuracy_index[acc_val]] = rmse\n",
    "\n",
    "        # Add the slice to the list\n",
    "        slice_df = pd.DataFrame(slice_matrix)\n",
    "        all_mean_dfs = pd.concat([all_mean_dfs, slice_df], ignore_index=True)   \n",
    "             \n",
    "        # Add list of PSS_Accuracy to mega list\n",
    "        final_pred_pss_accuracy = pd.concat([final_pred_pss_accuracy, Pred_PSS_Accuracy], ignore_index=True)\n",
    "    \n",
    "# Save the accumulated results to CSV\n",
    "final_pred_pss_accuracy.to_csv(\"Real_participants_pred_pss_accuracy_100_400.tsv\", index=False,  sep='\\t')\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c27721",
   "metadata": {},
   "source": [
    "\n",
    "# If all files have been generated run from here on:\n",
    "<a id=\"skip-here\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a118188c-2ac6-4ed3-88e2-dcea29378918",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#REFERENCE FILES:\n",
    "ref_diff_df = pd.read_csv(\"/PATH/Reference_diff_df_100_400.csv\")  \n",
    "\n",
    "#NULL FILES\n",
    "null_pred_pss_accuracy = pd.read_csv(\"/PATH/10000_null_simulations_100_400_pred_pss_accuracy.csv\")  \n",
    "null_all_mean_dfs = np.load(\"/PATH/10000_null_simulations_100_400_all_mean_rmse_plot.npy\")\n",
    "null_diff_df = pd.read_csv(\"/PATH/Null_diff_df_100_400.csv\")  \n",
    "\n",
    "## SIMULATED PARTICIPANT FILES:\n",
    "sim_participant_pred_pss_accuracy = pd.read_csv('/PATH/10000_simulated_participants_100_400_pred_pss_accuracy.tsv', sep='\\t')\n",
    "sim_participant_all_mean_df = np.load(\"/PATH/10000_simulated_participants_100_400_all_mean_rmse_plot.npy\")\n",
    "sim_participant_diff_df = pd.read_csv(\"/PATH/10000_simulated_participants_diff_df_100_400.csv\")  \n",
    "\n",
    "## SIMULATED PARTICIPANT FILES:\n",
    "real_participant_pred_pss_accuracy = pd.read_csv('/PATH/Real_participants_pred_pss_accuracy_100_400.tsv', sep='\\t')\n",
    "print(real_participant_pred_pss_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3194d2e7-d100-40cf-9106-661bcb0f4e43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "True_and_Pred = real_participant_pred_pss_accuracy\n",
    "\n",
    "# Ensure all PSS values and accuracy values are included\n",
    "pss_value_list = list(range(100, 401, 50))  # [100, 150, 200, 250, 300, 350, 400]\n",
    "accuracy_list = [round(i * 0.1, 1) for i in range(0, 11)]  # [0.0, 0.1, 0.2, ..., 1.0]\n",
    "\n",
    "fig_width_cm = 8\n",
    "fig_height_cm = 6\n",
    "fig_size_in = (fig_width_cm / 2.54, fig_height_cm / 2.54)\n",
    "\n",
    "# Frequency heatmap\n",
    "grouped_pred = True_and_Pred.groupby([\"Pred_PSS_Value\", \"Pred_Accuracy\"]).size().reset_index(name=\"Frequency\")\n",
    "heatmap_data_pred = grouped_pred.pivot(index=\"Pred_PSS_Value\", columns=\"Pred_Accuracy\", values=\"Frequency\")\n",
    "\n",
    "# Reindex without filling (leave blanks for missing data)\n",
    "heatmap_data_pred = heatmap_data_pred.reindex(index=pss_value_list[::-1], columns=accuracy_list)\n",
    "\n",
    "plt.figure(figsize=fig_size_in, dpi=300)\n",
    "ax = sns.heatmap(\n",
    "    heatmap_data_pred, \n",
    "    cmap=real_participant_cmap, \n",
    "    annot=True, \n",
    "    fmt=\".0f\", \n",
    "    vmin=0, \n",
    "    vmax=4, \n",
    "    annot_kws={\"size\": 5}, \n",
    "    cbar_kws={'label': 'Frequency of sample'}\n",
    ")\n",
    "full_accuracy_vals = np.arange(0, 101, 10)\n",
    "accuracy_percent_labels = [int(x) for x in full_accuracy_vals]\n",
    "\n",
    "ax.set_xticks(np.arange(len(accuracy_list)) + 0.5)\n",
    "ax.set_xticklabels([int(x*100) for x in accuracy_list], fontsize=7)\n",
    "ax.set_yticklabels(ax.get_yticklabels(), fontsize=7)\n",
    "ax.set_xlabel(\"Predicted Accuracy (%)\", fontsize=7)\n",
    "ax.set_ylabel(\"Predicted PSS Value (ms)\", fontsize=7)\n",
    "ax.collections[0].colorbar.ax.tick_params(labelsize=7)\n",
    "#plt.title(\"Predicted Accuracy vs Predicted PSS Value\", fontsize=7)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"Real_participants_100_400_pred_pss_accuracy.png\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Mean RMSE heatmap\n",
    "mean_rmse = True_and_Pred.groupby(['Pred_PSS_Value', 'Pred_Accuracy'])['RMSE'].mean().unstack()\n",
    "\n",
    "# Reindex without filling\n",
    "mean_rmse = mean_rmse.reindex(index=pss_value_list[::-1], columns=accuracy_list)\n",
    "\n",
    "plt.figure(figsize=fig_size_in, dpi=300)\n",
    "ax2 = sns.heatmap(\n",
    "    mean_rmse, \n",
    "    cmap=real_participant_cmap, \n",
    "    annot=True, \n",
    "    fmt='.1f', \n",
    "    vmin=0, \n",
    "    vmax=30, \n",
    "    annot_kws={\"size\": 5}, \n",
    "    cbar_kws={'label': 'Mean RMSE'}\n",
    ")\n",
    "ax2.set_xticks(np.arange(len(accuracy_list)) + 0.5)\n",
    "ax2.set_xticklabels([int(x*100) for x in accuracy_list], fontsize=7)\n",
    "ax2.set_yticklabels(ax2.get_yticklabels(), fontsize=7)\n",
    "ax2.set_xlabel(\"Predicted Accuracy (%)\", fontsize=7)\n",
    "ax2.set_ylabel(\"Predicted PSS Value (ms)\", fontsize=7)\n",
    "ax2.collections[0].colorbar.ax.tick_params(labelsize=7)\n",
    "#plt.title('Mean RMSE for Real Participants', fontsize=7)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"Real_participants_100_400_all_mean_rmse_plot.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6ddab8-19c2-466e-86f3-fab20c709c71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize an empty list to store the result\n",
    "vector_across_slices = []\n",
    "\n",
    "# Loop over each (y, x) position\n",
    "for y in range(null_all_mean_dfs.shape[1]):  # 12 rows\n",
    "    row = []\n",
    "    for x in range(null_all_mean_dfs.shape[2]):  # 11 columns\n",
    "        # Get all values across slices for the current (y, x) position\n",
    "        values_at_pos = null_all_mean_dfs[:, y, x].tolist()  # Values from axis 0 (across slices)\n",
    "        row.append(values_at_pos)\n",
    "    vector_across_slices.append(row)\n",
    "\n",
    "# Convert the list of lists into a pandas DataFrame\n",
    "df_vector_across_slices = pd.DataFrame(vector_across_slices)\n",
    "df_vector_across_slices.index = pred_pss_values  # Assuming `pred_pss_values` is the list of index labels\n",
    "df_vector_across_slices.columns = pred_accuracies  # Assuming `pred_accuracies` is the list of column labels\n",
    "\n",
    "#If plotting - shows disribution and participant as line\n",
    "real_participant_pred_pss_accuracy = compute_p_values(real_participant_pred_pss_accuracy, df_vector_across_slices, plotting=True)\n",
    "#Saves p-values into the Pred_PSS_Accuracy_drop_real file\n",
    "real_participant_pred_pss_accuracy.to_csv('Real_participants_pred_pss_accuracy_100_400_pvals.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471fabe0-3f50-4814-afa0-e740004526e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Assuming p_values is defined\n",
    "p_values = real_participant_pred_pss_accuracy['p_value']\n",
    "\n",
    "# Create the main histogram\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.histplot(p_values, bins=100, kde=False, color='#e54e40', alpha=0.6)\n",
    "\n",
    "# Set plot labels and title\n",
    "plt.title('Distribution of P-values')\n",
    "plt.xlabel('P-value')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Create inset axes\n",
    "ax_inset = inset_axes(plt.gca(), width=\"60%\", height=\"40%\", loc=\"upper right\")\n",
    "\n",
    "# Plot the zoomed-in region\n",
    "sns.histplot(p_values, bins=100, kde=False, color='#e54e40', alpha=0.6, ax=ax_inset)\n",
    "ax_inset.set_xlim(0, 0.08)\n",
    "#ax_inset.set_ylim(0, None)  # Adjust if needed\n",
    "ax_inset.set_xticks([0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07])\n",
    "#ax_inset.xlabel('P-value')\n",
    "#ax_inset.ylabel('Frequency')\n",
    "\n",
    "ax_inset.set_yticks([])  \n",
    "\n",
    "# Show plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf196f0-b21f-4eee-9898-861f02aee27e",
   "metadata": {},
   "source": [
    "Mean RMSE across paricipants, sig vs non sig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10dd447d-6310-4b80-86ae-b638b54685d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the full range of PSS and Accuracy values\n",
    "pss_value_list = list(range(100, 401, 50))  # [100, 150, 200, 250, 300, 350, 400]\n",
    "accuracy_list = [round(i * 0.1, 1) for i in range(0, 11)]  # [0.4, 0.5, 0.6, ..., 1.0]\n",
    "\n",
    "# Separate participants into two groups based on p-value\n",
    "significant_participants = real_participant_pred_pss_accuracy[real_participant_pred_pss_accuracy['p_value'] < 0.05]\n",
    "non_significant_participants = real_participant_pred_pss_accuracy[real_participant_pred_pss_accuracy['p_value'] >= 0.05]\n",
    "\n",
    "# Calculate the mean RMSE for each group\n",
    "mean_rmse_significant = significant_participants.groupby(['Pred_PSS_Value', 'Pred_Accuracy'])['RMSE'].mean().unstack()\n",
    "mean_rmse_non_significant = non_significant_participants.groupby(['Pred_PSS_Value', 'Pred_Accuracy'])['RMSE'].mean().unstack()\n",
    "\n",
    "# Reindex to include all PSS and Accuracy values with missing values as NaN\n",
    "mean_rmse_significant = mean_rmse_significant.reindex(index=pss_value_list[::-1], columns=accuracy_list)\n",
    "mean_rmse_non_significant = mean_rmse_non_significant.reindex(index=pss_value_list[::-1], columns=accuracy_list)\n",
    "\n",
    "# Plot the heatmap for significant participants (p < 0.05)\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(mean_rmse_significant, cmap=real_participant_cmap, annot=True, fmt='.1f', cbar_kws={'label': 'Mean RMSE'}, vmin=0, vmax=30)\n",
    "plt.title('Heatmap of Mean RMSE for Participants with p < 0.05')\n",
    "plt.xlabel('Pred Accuracy')\n",
    "plt.ylabel('Pred PSS Value')\n",
    "plt.show()\n",
    "\n",
    "# Plot the heatmap for non-significant participants (p >= 0.05)\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(mean_rmse_non_significant, cmap=real_participant_cmap, annot=True, fmt='.1f', cbar_kws={'label': 'Mean RMSE'}, vmin=0, vmax=30)\n",
    "plt.title('Heatmap of Mean RMSE for Participants with p >= 0.05')\n",
    "plt.xlabel('Pred Accuracy')\n",
    "plt.ylabel('Pred PSS Value')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8fd817-cc4f-4cc9-a1f8-ad79e97c2c6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "significant_participants = real_participant_pred_pss_accuracy[real_participant_pred_pss_accuracy['p_value'] < 0.05]\n",
    "non_significant_participants = real_participant_pred_pss_accuracy[real_participant_pred_pss_accuracy['p_value'] >= 0.05]\n",
    "\n",
    "# Set the number of rows and columns for the grid of subplots\n",
    "num_rows = df_vector_across_slices.shape[0]\n",
    "num_cols = df_vector_across_slices.shape[1]\n",
    "\n",
    "# Calculate the min/max values for the x and y axis ranges\n",
    "min_xvalue = 0\n",
    "max_xvalue = math.ceil(np.nanmax([item for sublist in df_vector_across_slices.values for item in sublist]) + 0.05)\n",
    "max_yvalue = 0\n",
    "\n",
    "# Find max frequency for scaling y-axis\n",
    "for cell_row in range(num_rows):\n",
    "    for cell_col in range(num_cols):\n",
    "        cell_values = df_vector_across_slices.iloc[cell_row, cell_col]\n",
    "        hist, bin_edges = np.histogram(cell_values, bins=50, range=(min_xvalue, max_xvalue))\n",
    "        max_yvalue = max(max_yvalue, hist.max())\n",
    "\n",
    "# Create figure and axes for subplots\n",
    "fig, axes = plt.subplots(nrows=num_rows, ncols=num_cols, figsize=(15,10))\n",
    "\n",
    "# Loop through each subplot to create histograms\n",
    "for cell_row in range(num_rows):\n",
    "    for cell_col in range(num_cols):\n",
    "        cell_values = df_vector_across_slices.iloc[cell_row, cell_col]\n",
    "        ax = axes[num_rows - 1 - cell_row, cell_col]  # Flip the row order\n",
    "\n",
    "        # Plot the histogram for the null distribution\n",
    "        sns.histplot(cell_values, bins=50, kde=False, color='#5085E1', edgecolor='#5085E1', alpha=0.7, ax=ax, \n",
    "                     binrange=(min_xvalue, max_xvalue))\n",
    "\n",
    "        # Overlay the RMSE values for significant and non-significant participants\n",
    "        significant_rmse_values = significant_participants[\n",
    "            (significant_participants['Pred_PSS_Value'] == pred_pss_values[cell_row]) &\n",
    "            (significant_participants['Pred_Accuracy'] == pred_accuracies[cell_col])\n",
    "        ]['RMSE']\n",
    "        non_significant_rmse_values = non_significant_participants[\n",
    "            (non_significant_participants['Pred_PSS_Value'] == pred_pss_values[cell_row]) &\n",
    "            (non_significant_participants['Pred_Accuracy'] == pred_accuracies[cell_col])\n",
    "        ]['RMSE']\n",
    "\n",
    "        # Plot vertical lines for significant RMSE values in red\n",
    "        for rmse_value in significant_rmse_values:\n",
    "            ax.vlines(rmse_value, 0, max_yvalue, color='#e54e40', alpha=0.8, linewidth=1.5, label='Significant')\n",
    "\n",
    "        # Plot vertical lines for non-significant RMSE values in blue\n",
    "        for rmse_value in non_significant_rmse_values:\n",
    "            ax.vlines(rmse_value, 0, max_yvalue, color='#e54e40', alpha=0.8, linewidth=1.5, linestyles= 'dashed',label='Non-Significant')\n",
    "        \n",
    "        # Remove axis labels and ticks for a clean look\n",
    "        ax.set_xticks([]) \n",
    "        ax.set_yticks([]) \n",
    "        ax.set_ylabel('')\n",
    "\n",
    "        # Set axis limits\n",
    "        ax.set_xlim(min_xvalue, max_xvalue)\n",
    "        ax.set_ylim(0, max_yvalue)\n",
    "\n",
    "        # Clean grid appearance\n",
    "        if cell_col == num_cols - 1:\n",
    "            ax.spines['top'].set_visible(True)\n",
    "            ax.spines['right'].set_visible(True)\n",
    "            ax.spines['left'].set_visible(True)\n",
    "            ax.spines['bottom'].set_visible(True)\n",
    "        else:\n",
    "            ax.spines['top'].set_visible(True)\n",
    "            ax.spines['right'].set_visible(False)\n",
    "            ax.spines['left'].set_visible(True)\n",
    "            ax.spines['bottom'].set_visible(True)\n",
    "\n",
    "        if cell_row == 0 and cell_col == 0:\n",
    "            ax.set_xticks(np.linspace(min_xvalue, max_xvalue, num=2))\n",
    "        else:\n",
    "            ax.set_xticks([])\n",
    "\n",
    "        if cell_col == 0 and cell_row == 0:\n",
    "            ax.set_yticks(np.linspace(0, max_yvalue, num=2))\n",
    "\n",
    "# Left Y-axis (Pred_PSS_Value)\n",
    "ax_left = fig.add_axes([0.065, 0.1, 0.01, 0.75])\n",
    "ax_left.set_xticks([])  \n",
    "ax_left.tick_params(axis=\"y\", direction=\"out\", length=12, width=1.2)\n",
    "ax_left.set_yticks(np.arange(num_rows) + 0.5)\n",
    "ax_left.set_yticklabels(pred_pss_values, fontsize=10, ha='right')\n",
    "ax_left.set_ylabel(\"Predicted PSS Value\\nNumber of instances\", fontsize=12, labelpad=20)\n",
    "ax_left.tick_params(left=False, labelleft=True, right=False, labelright=False)\n",
    "ax_left.spines['top'].set_visible(False)\n",
    "ax_left.spines['bottom'].set_visible(False)\n",
    "ax_left.spines['left'].set_visible(False)\n",
    "ax_left.spines['right'].set_visible(False)\n",
    "\n",
    "# Bottom X-axis (Pred_Accuracy)\n",
    "ax_bottom = fig.add_axes([0.16, 0.06, 0.7, 0.01])\n",
    "ax_bottom.set_yticks([])  \n",
    "ax_bottom.set_xticks(np.arange(num_cols))\n",
    "ax_bottom.set_xticklabels(pred_accuracies, rotation=0, fontsize=10, ha='center')\n",
    "ax_bottom.set_xlabel(\"RMSE\\nPredicted Accuracy\", fontsize=12, labelpad=15)\n",
    "ax_bottom.tick_params(axis=\"x\", direction=\"inout\", length=6, width=1.2)\n",
    "ax_bottom.spines['top'].set_visible(False)\n",
    "ax_bottom.spines['bottom'].set_visible(False)\n",
    "ax_bottom.spines['left'].set_visible(False)\n",
    "ax_bottom.spines['right'].set_visible(False)\n",
    "\n",
    "# Adjust layout to fit the grid tightly\n",
    "plt.subplots_adjust(wspace=0, hspace=0)\n",
    "plt.savefig(\"Real_participants_overlayed_vertical_line_null_distribution_plots.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5d450d",
   "metadata": {},
   "source": [
    "### âž• Additional Analyses: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256640db-07dc-4b64-99e8-b806941082b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_auroc_shdt(participant_data, participant_id):\n",
    "    # Calculate accuracy for each trial (1 = correct, 0 = incorrect)\n",
    "    participant_data = participant_data.copy()\n",
    "    \n",
    "    # Extract test variable (confidence) and state variable (accuracy)\n",
    "    y_true = participant_data['Recode_Accuracy']\n",
    "    y_scores = participant_data['Confidence']\n",
    "    \n",
    "    # Compute ROC curve\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    return {'Participant': participant_id, 'fpr': fpr, 'tpr': tpr, 'roc_auc': roc_auc}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cfbb34b-18f0-4278-a1f8-d741fdd612b3",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Loop over each file in the directory\n",
    "data_directory = '/PATH/SUBJECT_DATA/'\n",
    "\n",
    "real_participant_pred_pss_accuracy = pd.read_csv('/PATH/Real_participants_pred_pss_accuracy_100_400.tsv', sep='\\t')\n",
    "\n",
    "roc_results = []\n",
    "plt.figure(figsize=(8, 6))  # Prepare figure for group-level ROC plot\n",
    "\n",
    "#âš ï¸ IF EXCLUDING PARTICIPANTS WHO HAVE RANDOM BUTTON PRESS AND PARALLEL LINES\n",
    "excluded_ids = set(exclude_ids_parallel) | set(exclude_ids_random)\n",
    "\n",
    "for filename in os.listdir(data_directory):\n",
    "    if filename.endswith('.tsv'):  # Process only TSV files\n",
    "        # Extract participant ID: Keep only the numeric part (e.g., 'sub-005d' â†’ '005')\n",
    "        match = re.search(r'sub-(\\d+)', filename)\n",
    "        if not match:\n",
    "            continue  # Skip files that do not match the expected format\n",
    "        participant_id = match.group(1)  # Extract the numeric part (e.g., '005')\n",
    "\n",
    "          # âš ï¸ Exclude if participant_id is in either exclusion set\n",
    "        if participant_id in excluded_ids:\n",
    "            print(f\"Skipping excluded participant: {participant_id}\")\n",
    "            continue\n",
    "        \n",
    "        #print(participant_id)\n",
    "        # Read the participant's data file\n",
    "        participants_df = pd.read_csv(os.path.join(data_directory, filename), sep='\\t',  skiprows=1)  \n",
    "        participants_df.columns = [col.strip().replace('\\t', '') for col in participants_df.columns]\n",
    "    \n",
    "        participants_df = participants_df[participants_df['Staircase_name'] != 'Training']\n",
    "        participants_df = participants_df[participants_df['Staircase_name'] != 'Post_task_question']\n",
    "        # Clean column names (remove tabs, strip spaces)\n",
    "        \n",
    "        #print(participants_df)\n",
    "        \n",
    "        participants_df['Trial'] = participants_df['Trial'].astype(int)\n",
    "        participants_df['Modified_Staircase_name'] = participants_df['Staircase_name'].str[:-2]\n",
    "\n",
    "        # Recode trial numbers\n",
    "        participants_df.loc[:, 'Recode_Trial'] = np.where(\n",
    "            participants_df['Staircase_name'].str.contains('400'),\n",
    "            participants_df['Trial'], \n",
    "            participants_df['Trial'] + 15\n",
    "        )\n",
    "        \n",
    "       \n",
    "        \n",
    "        pss_row = real_participant_pred_pss_accuracy.loc[\n",
    "            real_participant_pred_pss_accuracy['Participant_ID'].apply(lambda x: str(x).zfill(3)) == str(participant_id).zfill(3),\n",
    "            'Pred_PSS_Value'\n",
    "        ]\n",
    "        \n",
    "        pss_value = pss_row.values[0]\n",
    "        print('PSS:', pss_value)\n",
    "        \n",
    "        accuracy_value = real_participant_pred_pss_accuracy.loc[\n",
    "            real_participant_pred_pss_accuracy['Participant_ID'].apply(lambda x: str(x).zfill(3)) == str(participant_id).zfill(3),\n",
    "            'Pred_Accuracy'].values[0]\n",
    "\n",
    "        print('Accuracy:', accuracy_value)\n",
    "\n",
    "        # Function to determine correctness\n",
    "        def classify_response(row):\n",
    "            if row['Response_code'] == -1:  # 'same time'\n",
    "                return 1 if row['Current Delay'] == pss_value else 0\n",
    "            elif row['Response_code'] == 0:  # 'before'\n",
    "                return 1 if row['Current Delay'] < pss_value else 0\n",
    "            elif row['Response_code'] == 1:  # 'after'\n",
    "                return 1 if row['Current Delay'] > pss_value else 0\n",
    "            return None\n",
    "\n",
    "        # Apply classification\n",
    "        participants_df['Recode_Accuracy'] = participants_df.apply(classify_response, axis=1)\n",
    "\n",
    "        # Create trial-by-trial DataFrame\n",
    "        trial_data = participants_df[['Staircase_name', 'Recode_Trial', 'Current Delay', 'Button', 'Response_code', 'Recode_Accuracy', 'Confidence']]\n",
    "\n",
    "        #print(trial_data)\n",
    "        print('Recoded Accuracy:', sum(trial_data['Recode_Accuracy']) / 60, '\\n')\n",
    "        \n",
    "        #Calculate ROC\n",
    "        roc_result = calculate_auroc_shdt(trial_data, participant_id)\n",
    "        roc_results.append(roc_result)\n",
    "        plt.plot(roc_result['fpr'], roc_result['tpr'], alpha = 0.4, label=f'Participant {participant_id} (AUC = {roc_result[\"roc_auc\"]:.2f})')        \n",
    "        #print(roc_result)\n",
    "\n",
    "        \n",
    "# Finalize group-level ROC plot\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='black', alpha = 1)  # Diagonal line (random classifier)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves')\n",
    "#plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "df_auc = pd.DataFrame([{'Participant_ID': r['Participant'], 'AUC': r['roc_auc']} for r in roc_results])\n",
    "\n",
    "#display(HTML(df_auc.to_html()))\n",
    "# Create a box plot for AUC values\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.boxplot(y=df_auc['AUC'])\n",
    "plt.title('Distribution of ROC AUC Values')\n",
    "plt.ylabel('AUC')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "df_auc['Participant_ID'] = df_auc['Participant_ID'].astype(int)  # or .astype(int) if needed\n",
    "real_participant_pred_pss_accuracy['Participant_ID'] = real_participant_pred_pss_accuracy['Participant_ID'].astype(int)  # or \n",
    "real_participant_pred_pss_accuracy_auc = pd.merge(real_participant_pred_pss_accuracy, df_auc, on='Participant_ID', how='left')\n",
    "print(real_participant_pred_pss_accuracy_auc)\n",
    "\n",
    "# Calculate and plot correlation between AUC and real_participant_pred_pss_accuracy\n",
    "correlation, p_value = pearsonr(real_participant_pred_pss_accuracy_auc['AUC'], real_participant_pred_pss_accuracy_auc['Pred_Accuracy'])\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.scatterplot(x=real_participant_pred_pss_accuracy_auc['AUC'], y=real_participant_pred_pss_accuracy_auc['Pred_Accuracy'])\n",
    "plt.xlabel('AUC')\n",
    "plt.ylabel('Real Participant PSS Accuracy')\n",
    "plt.title(f'Correlation: {correlation:.2f} (p = {p_value:.3f})')\n",
    "plt.show()\n",
    "\n",
    "print(f'Pearson correlation: {correlation:.2f}, p-value: {p_value:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ebe97f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_participant_pred_pss_accuracy_auc['AUC'].mean()\n",
    "real_participant_pred_pss_accuracy_auc['AUC'].std()\n",
    "real_participant_pred_pss_accuracy_auc['AUC'].min()\n",
    "real_participant_pred_pss_accuracy_auc['AUC'].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8407905b-13a7-4311-b3a8-722866b48448",
   "metadata": {},
   "source": [
    "DIFF PER TRIAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b969f8-c3ff-498f-8813-bbf6e48d720d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "participant_per_trial_diff_df = pd.DataFrame()\n",
    "\n",
    "for filename in tqdm(os.listdir(data_directory)):\n",
    "    if filename.endswith('.tsv'):  # Process only TSV files\n",
    "        # Extract participant ID: Keep only the numeric part (e.g., 'sub-005d' â†’ '005')\n",
    "        match = re.search(r'sub-(\\d+)', filename)\n",
    "        if not match:\n",
    "            continue  # Skip files that do not match the expected format\n",
    "        participant_id = match.group(1)  # Extract the numeric part (e.g., '005')\n",
    "        #print(participant_id)\n",
    "        # Read the participant's data file\n",
    "        participants_df = pd.read_csv(os.path.join(data_directory, filename), sep='\\t',  skiprows=1)  \n",
    "        participants_df.columns = [col.strip().replace('\\t', '') for col in participants_df.columns]\n",
    "    \n",
    "        participants_df = participants_df[participants_df['Staircase_name'] != 'Training']\n",
    "        participants_df = participants_df[participants_df['Staircase_name'] != 'Post_task_question']\n",
    "        participants_df['Modified_Staircase_name'] = participants_df['Staircase_name'].str[:-2]\n",
    "        participants_df['Trial'] = participants_df['Trial'].astype(int)\n",
    "        \n",
    "        # Recode trial numbers\n",
    "        participants_df.loc[:, 'Recode_Trial'] = np.where(\n",
    "            participants_df['Staircase_name'].str.contains('400'),\n",
    "            participants_df['Trial'], \n",
    "            participants_df['Trial'] + 15\n",
    "        )\n",
    "        \n",
    "         # For 400ms (trials 1-15)\n",
    "        mean_delays_400 = participants_df[participants_df['Staircase_name'].str.contains('400')] \\\n",
    "                            .groupby('Recode_Trial')['Current Delay'].mean()\n",
    "\n",
    "        # For 100ms (trials 16-30)\n",
    "        mean_delays_100 = participants_df[participants_df['Staircase_name'].str.contains('100')] \\\n",
    "                            .groupby('Recode_Trial')['Current Delay'].mean()\n",
    "\n",
    "        # Combine the two mean delay series (400_1, 400_2, and 100_1, 100_2) into one DataFrame\n",
    "        mean_delays = mean_delays_400.append(mean_delays_100)\n",
    "\n",
    "        # Reindex the trials: 1-15 for 400ms, 16-30 for 100ms\n",
    "        mean_delays.index = np.arange(1, 31)\n",
    "                \n",
    "        participant_data = {'Participant_ID': participant_id}\n",
    "        \n",
    "        # Compute differences for each trial\n",
    "        for trial in range(1, 16):\n",
    "            value_400 = mean_delays[trial]\n",
    "            value_100 = mean_delays[trial + 15]\n",
    "            trial_diff = value_400 - value_100\n",
    "\n",
    "            # Store values in dictionary\n",
    "            #participant_data[f'trial_{trial}_start'] = start_value\n",
    "            #participant_data[f'trial_{trial}_end'] = end_value\n",
    "            participant_data[f'trial_{trial}_diff'] = trial_diff\n",
    "\n",
    "        # Append the dictionary as a single row to the DataFrame\n",
    "        participant_per_trial_diff_df = participant_per_trial_diff_df.append(participant_data, ignore_index=True)\n",
    "\n",
    "\n",
    "display(HTML(participant_per_trial_diff_df.head().to_html()))       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbe5213-cda4-463c-9685-195d2d56193c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sim_participants_per_trial_diff_df = pd.DataFrame()\n",
    "\n",
    "sim_participants_file = f'/PATH/10000_simulated_participants_100_400.tsv'\n",
    "sim_participants_df = pd.read_csv(sim_participants_file, sep='\\t')\n",
    "\n",
    "sim_participants_df['Trial'] = sim_participants_df['Trial'].astype(int)\n",
    "sim_participants_df['Modified_Staircase_name'] = sim_participants_df['Staircase_name'].str[:-2]\n",
    "\n",
    "# Recode trial numbers\n",
    "sim_participants_df.loc[:, 'Recode_Trial'] = np.where(\n",
    "    sim_participants_df['Staircase_name'].str.contains('400'),\n",
    "    sim_participants_df['Trial'], \n",
    "    sim_participants_df['Trial'] + 15\n",
    ")\n",
    "\n",
    "# Get participant IDs\n",
    "participant_ids = sim_participants_df['Participant_ID'].unique().tolist()\n",
    "\n",
    "# Loop over each participant ID\n",
    "for participant_id in participant_ids[0:10001]:\n",
    "    participants_df = sim_participants_df[sim_participants_df['Participant_ID'] == participant_id]\n",
    "    participants_df['Trial'] = participants_df['Trial'].astype(int)\n",
    "    participants_df['Modified_Staircase_name'] = participants_df['Staircase_name'].str[:-2]\n",
    "    \n",
    "    # Recode trial numbers\n",
    "    participants_df.loc[:, 'Recode_Trial'] = np.where(\n",
    "        participants_df['Staircase_name'].str.contains('400'),\n",
    "        participants_df['Trial'], \n",
    "        participants_df['Trial'] + 15\n",
    "    )\n",
    "    \n",
    "    # Get participant IDs\n",
    "    participant_ids = participants_df['Participant_ID'].unique().tolist()\n",
    "    \n",
    "\n",
    "     # For 400ms (trials 1-15)\n",
    "    mean_delays_400 = participants_df[participants_df['Staircase_name'].str.contains('400')] \\\n",
    "                        .groupby('Recode_Trial')['Current Delay'].mean()\n",
    "\n",
    "    # For 100ms (trials 16-30)\n",
    "    mean_delays_100 = participants_df[participants_df['Staircase_name'].str.contains('100')] \\\n",
    "                        .groupby('Recode_Trial')['Current Delay'].mean()\n",
    "\n",
    "    # Combine the two mean delay series (400_1, 400_2, and 100_1, 100_2) into one DataFrame\n",
    "    mean_delays = mean_delays_400.append(mean_delays_100)\n",
    "\n",
    "    # Reindex the trials: 1-15 for 400ms, 16-30 for 100ms\n",
    "    mean_delays.index = np.arange(1, 31)\n",
    "\n",
    "                \n",
    "    participant_data = {'Participant_ID': participant_id}\n",
    "\n",
    "    # Compute differences for each trial\n",
    "    for trial in range(1, 16):\n",
    "        value_400 = mean_delays[trial]\n",
    "        value_100 = mean_delays[trial + 15]\n",
    "        trial_diff = value_400 - value_100\n",
    "\n",
    "        # Store values in dictionary\n",
    "        #participant_data[f'trial_{trial}_start'] = start_value\n",
    "        #participant_data[f'trial_{trial}_end'] = end_value\n",
    "        participant_data[f'trial_{trial}_diff'] = trial_diff\n",
    "\n",
    "    # Append the dictionary as a single row to the DataFrame\n",
    "    sim_participants_per_trial_diff_df = sim_participants_per_trial_diff_df.append(participant_data, ignore_index=True)\n",
    "\n",
    "\n",
    "display(HTML(sim_participants_per_trial_diff_df.head().to_html()))       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afdb9dbb-7ea3-42aa-bc5e-968bfcb8d98e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot KDEs in a 3x5 subplot\n",
    "fig, axes = plt.subplots(3, 5, figsize=(15, 9))\n",
    "fig.tight_layout(pad=4.0)\n",
    "\n",
    "for trial, ax in zip(range(1, 16), axes.flatten()):\n",
    "    sns.kdeplot(null_per_trial_diff_df[f'trial_{trial}_diff'], color=\"#5085E1\", ax=ax)\n",
    "    sns.kdeplot(sim_participants_per_trial_diff_df[f'trial_{trial}_diff'], color=\"#e86a36\", ax=ax)\n",
    "    sns.kdeplot(participant_per_trial_diff_df[f'trial_{trial}_diff'], color=\"#e54e40\",ax=ax)\n",
    "    ax.set_xlabel(f\"Difference at Trial {trial} (ms)\", fontsize=10)\n",
    "    ax.set_ylabel(\"Density\", fontsize=10)\n",
    "    ax.set_title(f\"Trial {trial}\", fontsize=12)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d628ce6-7255-478a-9874-96e04c32d51e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#display(HTML(null_per_trial_diff_df.head().to_html()))       \n",
    "threshold = 100\n",
    "\n",
    "filtered_participants = null_per_trial_diff_df[null_per_trial_diff_df['trial_3_diff'] < threshold]\n",
    "print(f\"Null with trial_3_diff < {threshold}:\")\n",
    "#print(filtered_participants[['Participant_ID', 'trial_3_diff']])\n",
    "print(len(filtered_participants[['Participant_ID', 'trial_3_diff']])/len(null_per_trial_diff_df) * 100)\n",
    "\n",
    "filtered_participants = null_per_trial_diff_df[null_per_trial_diff_df['trial_5_diff'] < threshold]\n",
    "print(f\"Null with trial_5_diff < {threshold}:\")\n",
    "#print(filtered_participants[['Participant_ID', 'trial_5_diff']])\n",
    "print(len(filtered_participants[['Participant_ID', 'trial_5_diff']])/len(null_per_trial_diff_df) * 100)\n",
    "\n",
    "\n",
    "filtered_participants = null_per_trial_diff_df[null_per_trial_diff_df['trial_7_diff'] < threshold]\n",
    "print(f\"Null with trial_7_diff < {threshold}:\")\n",
    "#print(filtered_participants[['Participant_ID', 'trial_7_diff']])\n",
    "print(len(filtered_participants[['Participant_ID', 'trial_7_diff']]) / len(null_per_trial_diff_df) * 100)\n",
    "\n",
    "filtered_participants = null_per_trial_diff_df[null_per_trial_diff_df['trial_9_diff'] < threshold]\n",
    "print(f\"Null with trial_9_diff < {threshold}:\")\n",
    "#print(filtered_participants[['Participant_ID', 'trial_9_diff']])\n",
    "print(len(filtered_participants[['Participant_ID', 'trial_9_diff']]) / len(null_per_trial_diff_df) * 100)\n",
    "\n",
    "filtered_participants = null_per_trial_diff_df[null_per_trial_diff_df['trial_11_diff'] < threshold]\n",
    "print(f\"Null with trial_11_diff < {threshold}:\")\n",
    "#print(filtered_participants[['Participant_ID', 'trial_11_diff']])\n",
    "print(len(filtered_participants[['Participant_ID', 'trial_11_diff']]) / len(null_per_trial_diff_df) * 100)\n",
    "\n",
    "filtered_participants = null_per_trial_diff_df[null_per_trial_diff_df['trial_13_diff'] < threshold]\n",
    "print(f\"Null with trial_13_diff < {threshold}:\")\n",
    "#print(filtered_participants[['Participant_ID', 'trial_13_diff']])\n",
    "print(len(filtered_participants[['Participant_ID', 'trial_13_diff']]) / len(null_per_trial_diff_df) * 100)\n",
    "\n",
    "filtered_participants = null_per_trial_diff_df[null_per_trial_diff_df['trial_15_diff'] < threshold]\n",
    "print(f\"Null with trial_15_diff < {threshold}:\")\n",
    "#print(filtered_participants[['Participant_ID', 'trial_15_diff']])\n",
    "print(len(filtered_participants[['Participant_ID', 'trial_15_diff']]) / len(null_per_trial_diff_df) * 100)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#display(HTML(sim_participants_per_trial_diff_df.head().to_html()))       \n",
    "#display(HTML(participant_per_trial_diff_df.head().to_html()))       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dea7eb2-1622-4689-a1a6-9ba9257af996",
   "metadata": {},
   "source": [
    "# Confidence Ratings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0daacf-c46e-416c-b696-ce17a3ca8f3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Add mean confidence ratings\n",
    "data_directory = '/PATH/SUBJECT_DATA/'\n",
    "conf_list = []  # List to store individual DataFrames\n",
    "conf_df = pd.DataFrame()\n",
    "\n",
    "#âš ï¸ IF EXCLUDING PARTICIPANTS WHO HAVE RANDOM BUTTON PRESS AND PARALLEL LINES\n",
    "excluded_ids = set(exclude_ids_parallel) | set(exclude_ids_random)\n",
    "\n",
    "for filename in os.listdir(data_directory):\n",
    "    if filename.endswith('.tsv'):  # Process only TSV files\n",
    "        # Extract participant ID: Keep only the numeric part (e.g., 'sub-005d' â†’ '005')\n",
    "        match = re.search(r'sub-(\\d+)', filename)\n",
    "        if not match:\n",
    "            continue  # Skip files that do not match the expected format\n",
    "        \n",
    "        participant_id = match.group(1)  # Extract the numeric part (e.g., '005')\n",
    "        \n",
    "        # Read the participant's data file\n",
    "        file_path = os.path.join(data_directory, filename)\n",
    "        participants_df = pd.read_csv(file_path, sep='\\t', skiprows=1)  \n",
    "        \n",
    "        # Clean column names\n",
    "        participants_df.columns = [col.strip().replace('\\t', '') for col in participants_df.columns]\n",
    "        \n",
    "        # Add a new column for Participant_ID\n",
    "        participants_df.insert(0, 'Participant_ID', participant_id)  # Insert as first column\n",
    "        \n",
    "        # Append to the list\n",
    "        conf_list.append(participants_df)\n",
    "\n",
    "conf_df = pd.concat(conf_list, ignore_index=True)\n",
    "conf_df['Participant_ID'] = conf_df['Participant_ID'].astype(int)\n",
    "\n",
    "# Calculate mean confidence for each participant\n",
    "mean_confidence = conf_df.groupby('Participant_ID')['Confidence'].mean().reset_index()\n",
    "#mean_confidence.rename(columns={'Confidence': 'SHDT_Mean_Confidence'}, inplace=True)\n",
    "real_participant_pred_pss_accuracy_auc_conf = real_participant_pred_pss_accuracy_auc = pd.merge(real_participant_pred_pss_accuracy_auc, mean_confidence, on='Participant_ID', how='left')\n",
    "real_participant_pred_pss_accuracy_auc_conf.to_csv('/PATH/Real_participant_pred_pss_accuracy_auc_conf.tsv', sep='\\t', index=False)\n",
    "\n",
    "print(real_participant_pred_pss_accuracy_auc_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759d4fba-f5f1-47b1-b675-81a93b7d20af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Add Subjective Reports\n",
    "posttask_subjective_ratings_list = []  # List to store individual DataFrames\n",
    "posttask_subjective_ratings_df = pd.DataFrame()\n",
    "for filename in os.listdir(data_directory):\n",
    "    if filename.endswith('.tsv'):  # Process only TSV files\n",
    "        # Extract participant ID: Keep only the numeric part (e.g., 'sub-005d' â†’ '005')\n",
    "        match = re.search(r'sub-(\\d+)', filename)\n",
    "        if not match:\n",
    "            continue  # Skip files that do not match the expected format\n",
    "        \n",
    "        participant_id = match.group(1)  # Extract the numeric part (e.g., '005')\n",
    "        \n",
    "        # Read the participant's data file\n",
    "        file_path = os.path.join(data_directory, filename)\n",
    "        participants_df = pd.read_csv(file_path, sep='\\t', skiprows=1, usecols=['Staircase_name', 'Trial', 'Current Delay'])  \n",
    "        \n",
    "        # Clean column names\n",
    "        participants_df.columns = [col.strip().replace('\\t', '') for col in participants_df.columns]\n",
    "        \n",
    "        # Add a new column for Participant_ID\n",
    "        participants_df.insert(0, 'Participant_ID', participant_id)  # Insert as first column\n",
    "\n",
    "        # Filter rows where 'Staircase_name' is 'Post_task_question'\n",
    "        participants_df = participants_df[participants_df['Staircase_name'] == 'Post_task_question']\n",
    "        \n",
    "        # Convert 'Current Delay' to integers\n",
    "        participants_df['Current Delay'] = participants_df['Current Delay'].astype(int)\n",
    "        participants_df.rename(columns={'Current Delay': 'Rating'}, inplace=True)\n",
    "\n",
    "        # Append to the list\n",
    "        posttask_subjective_ratings_list.append(participants_df)\n",
    "\n",
    "# Combine all DataFrames into a single one\n",
    "posttask_subjective_ratings_df = pd.concat(posttask_subjective_ratings_list, ignore_index=True)\n",
    "# Pivot the DataFrame\n",
    "posttask_subjective_ratings_df = posttask_subjective_ratings_df.pivot(index='Participant_ID', columns='Trial', values='Rating')\n",
    "# Reset index to make it a normal DataFrame\n",
    "posttask_subjective_ratings_df.reset_index(inplace=True)\n",
    "posttask_subjective_ratings_df.columns.name = None  # Remove the automatic name given to columns\n",
    "posttask_subjective_ratings_df['Participant_ID'] = posttask_subjective_ratings_df['Participant_ID'].astype(int)\n",
    "\n",
    "#merged = HDT.merge(posttask_subjective_ratings_df, on='Participant_ID', how='left')\n",
    "#display(HTML(merged.head().to_html()))\n",
    "\n",
    "\n",
    "# Get unique Question_Types\n",
    "question_types = ['TaskGeneral', 'Difficulty', 'Breathless']\n",
    "print(question_types)\n",
    "\n",
    "# Set up the plot style\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Set the number of subplots needed\n",
    "n = len(question_types)\n",
    "cols = 1  # Number of columns in the subplot grid\n",
    "rows = n  # Number of rows based on the number of Question_Types\n",
    "\n",
    "# Create a figure with subplots\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(15,5))\n",
    "axes = axes.flatten()  # Flatten axes for easy iteration\n",
    "\n",
    "# Loop over each Question_Type to create subplots\n",
    "for i, question in enumerate(question_types):\n",
    "    ax = axes[i] # Set up the plot style\n",
    "    sns.set(style=\"whitegrid\")\n",
    "    # Select the subplot\n",
    "    question_data = posttask_subjective_ratings_df[question]  # Filter data for the current Question_Type\n",
    "    \n",
    "    # Create a box plot (ensure 'Instance' is treated as a categorical variable)\n",
    "    sns.boxplot(\n",
    "        data=question_data,\n",
    "        #y=question,             # y-axis: Trial\n",
    "        #x=question,         # y-axis: Instance (grouped)\n",
    "        orient='h', \n",
    "        dodge = False, # Make the box plot horizontal\n",
    "        color='white', \n",
    "        linewidth = 1.5,\n",
    "        ax=ax,\n",
    "        showfliers=False,\n",
    "        width = 0.2\n",
    "    )   \n",
    "\n",
    "\n",
    "    # Add individual participant responses as a scatter plot (strip plot)\n",
    "    sns.stripplot(\n",
    "        data=question_data,\n",
    "        #y=question,             # y-axis: Trial\n",
    "        #x=question,     # y-axis: Instance (grouped)\n",
    "        #hue = 'Participant_ID',\n",
    "        #palette='viridis',\n",
    "        color='red',\n",
    "        orient='h',             # Align with the box plot horizontally\n",
    "        dodge=False,             # Separate the points by Instance\n",
    "        jitter=False,           # Add jitter to avoid overlap\n",
    "        size=6,                 # Size of the scatter points\n",
    "        alpha=1.0,              # Transparency for the points\n",
    "        ax=ax\n",
    "    )\n",
    "        \n",
    "    # Customize the subplot\n",
    "    ax.set_xlabel('', fontsize=12)\n",
    "    ax.set_ylabel((f\"{question}\"), fontsize=12)\n",
    "    ax.legend([], [], frameon=False)  # Remove redundant legend from individual plots\n",
    "    ax.grid(axis='y', linestyle='--', alpha=0.7)  \n",
    "    ax.set_xlim(-5, 105) \n",
    "plt.xlabel('Score')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt_name = 'CARDIAC_subjective_ratings.png'\n",
    "#plt_save_path = save_dir + '/'+ plt_name\n",
    "#plt.savefig(plt_save_path, dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49d2ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up color palette\n",
    "real_colors = ['#e0736f', '#e69fab', '#dc5746']  \n",
    "excluded_participants = set(exclude_ids_parallel) | set(exclude_ids_random)\n",
    "\n",
    "# Function to assign dot color\n",
    "def color_by_participant(pid):\n",
    "    return real_colors[1] if pid in excluded_participants else real_colors[2]\n",
    "\n",
    "# Define questions and styling\n",
    "question_types = ['TaskGeneral', 'Difficulty', 'Breathless']\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "\n",
    "fig_width_cm = 16\n",
    "fig_height_cm = 5\n",
    "fig_size_in = (fig_width_cm / 2.54, fig_height_cm / 2.54)\n",
    "fig, axes = plt.subplots(len(question_types), 1, figsize=(fig_size_in), gridspec_kw={'height_ratios': [1]*len(question_types)}, dpi=300)\n",
    "\n",
    "for i, question in enumerate(question_types):\n",
    "    ax = axes[i]\n",
    "\n",
    "    data = posttask_subjective_ratings_df[question].dropna()\n",
    "    participant_ids = posttask_subjective_ratings_df.loc[data.index, 'Participant_ID']\n",
    "\n",
    "    # --- KDE Plot (above boxplot) ---\n",
    "    ax_kde = ax.inset_axes([0, 1.15, 1, 0.2], sharex=ax)\n",
    "    sns.kdeplot(data, fill=True, alpha=0.3, color=real_colors[0], ax=ax_kde)\n",
    "    ax_kde.set_ylabel('')\n",
    "    ax_kde.set_xlabel('')\n",
    "    ax_kde.set_yticks([])\n",
    "    ax_kde.set_xticks([])\n",
    "    ax_kde.tick_params(bottom=False, labelbottom=False)  # Ensure no x ticks or labels\n",
    "    ax_kde.spines['bottom'].set_visible(False)\n",
    "    ax_kde.spines['top'].set_visible(False)\n",
    "    ax_kde.spines['left'].set_visible(False)\n",
    "    ax_kde.spines['right'].set_visible(False)\n",
    "    ax_kde.grid(False)\n",
    "\n",
    "    # --- Boxplot ---\n",
    "    sns.boxplot(\n",
    "        x=data,\n",
    "        orient='h',\n",
    "        color='white',\n",
    "        linewidth=1.5,\n",
    "        width=0.3,\n",
    "        showfliers=False,\n",
    "        ax=ax\n",
    "    )\n",
    "\n",
    "    # Dot coloring based on inclusion\n",
    "    dot_colors = [color_by_participant(pid) for pid in participant_ids]\n",
    "    jitter = np.random.uniform(-0.01, 0.01, size=len(data))\n",
    "    ax.scatter(data.values, jitter, c=dot_colors, edgecolor='gray', linewidth=0, s=25, alpha=0.5, zorder=5)\n",
    "\n",
    "    # Labeling and styling\n",
    "    ax.set_ylabel(question, fontsize=7)\n",
    "    ax.set_xlabel('', fontsize=7)\n",
    "    ax.set_xlim(-5, 105)\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xticks([0,10,20,30,40,50,60,70,80,90,100])\n",
    "    ax.tick_params(axis='x', labelsize=7)\n",
    "    ax.grid(False)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "\n",
    "# Legend for included/excluded\n",
    "legend_elements = [\n",
    "    Patch(facecolor=real_colors[2], edgecolor='gray', label='Included Participants'),\n",
    "    Patch(facecolor=real_colors[1], edgecolor='gray', label='Excluded Participants')\n",
    "]\n",
    "#fig.legend(handles=legend_elements, loc='upper right', fontsize=12, frameon=False)\n",
    "\n",
    "plt.xlabel('Score', fontsize=7)\n",
    "#plt.tight_layout(rect=[0, 0, 0.9, 1])\n",
    "sns.despine()\n",
    "plt.subplots_adjust(hspace=1.15)  # more vertical spacing between rows\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48c8b7c-0551-42f1-bd7c-f4f60ab888d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "display(HTML(merged.describe().to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5697ab5b-7095-4a40-8c25-d284e7aac70b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set plot style\n",
    "sns.set(style=\"white\")\n",
    "#print(merged)\n",
    "\n",
    "# Create scatter plot 1\n",
    "plt.figure(figsize=(10, 6))\n",
    "norm = plt.Normalize(0, 1)\n",
    "cbar_ticks = np.arange(0, 1.1, 0.1) \n",
    "fig, ax = plt.subplots()\n",
    "scatter = sns.scatterplot(\n",
    "    data=merged, \n",
    "    x='Difficulty', \n",
    "    y='SHDT_Mean_Confidence',\n",
    "    #size='Pred_PSS_Value',  # Scale dot size\n",
    "    hue='Pred_Accuracy',    # Color by participant\n",
    "    palette='viridis',\n",
    "    hue_norm=(0, 1),\n",
    "    alpha=1.0               # Transparency for better visibility\n",
    ")\n",
    "plt.xlim(0, 100)  \n",
    "plt.ylim(0,100)\n",
    "\n",
    "ax.legend_.remove()  # Remove legend\n",
    "\n",
    "\n",
    "S_HDT_pearsonR_conf_difficulty = pearsonr(merged['SHDT_Mean_Confidence'], merged['Difficulty'])\n",
    "print('S_HDT: Is confidence related to difficulty? \\n R =', S_HDT_pearsonR_conf_difficulty[0], '\\n P-value:',S_HDT_pearsonR_conf_difficulty[1], '\\n R2 = ', S_HDT_pearsonR_conf_difficulty[0]**2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e5d4ee-d9de-4c53-a15c-83e4c31acfeb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set plot style\n",
    "sns.set(style=\"white\")\n",
    "\n",
    "# Create scatter plot 1\n",
    "plt.figure(figsize=(10, 6))\n",
    "norm = plt.Normalize(0, 1)\n",
    "cbar_ticks = np.arange(0, 1.1, 0.1) \n",
    "fig, ax = plt.subplots()\n",
    "scatter = sns.scatterplot(\n",
    "    data=merged, \n",
    "    x='TaskGeneral', \n",
    "    y='SHDT_Mean_Confidence',\n",
    "    #size='Pred_PSS_Value',  # Scale dot size\n",
    "    hue='Pred_Accuracy',    # Color by participant\n",
    "    palette='viridis',\n",
    "    hue_norm=(0, 1),\n",
    "    alpha=1.0               # Transparency for better visibility\n",
    ")\n",
    "plt.xlim(0, 100)  \n",
    "plt.ylim(0,100)\n",
    "\n",
    "ax.legend_.remove()  # Remove legend\n",
    "\n",
    "\n",
    "S_HDT_pearsonR_conf_taskG = pearsonr(merged['SHDT_Mean_Confidence'], merged['TaskGeneral'])\n",
    "print('S_HDT: Is confidence related to how pleasant they found the task? \\n R =', S_HDT_pearsonR_conf_taskG[0], '\\n P-value:',S_HDT_pearsonR_conf_taskG[1], '\\n R2 = ', S_HDT_pearsonR_conf_taskG[0]**2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e956d78-1883-44cd-a787-eef7b21e96d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set plot style\n",
    "sns.set(style=\"white\")\n",
    "\n",
    "# Create scatter plot 1\n",
    "plt.figure(figsize=(10, 6))\n",
    "norm = plt.Normalize(0, 1)\n",
    "cbar_ticks = np.arange(0, 1.1, 0.1) \n",
    "fig, ax = plt.subplots()\n",
    "scatter = sns.scatterplot(\n",
    "    data=merged, \n",
    "    x=(merged['Pred_Accuracy'] * 100), \n",
    "    y='SHDT_Mean_Confidence',\n",
    "    #size='Pred_PSS_Value',  # Scale dot size\n",
    "    #hue='Participant_ID',    # Color by participant\n",
    "    #palette='viridis',\n",
    "    #hue_norm=(0, 1),\n",
    "    color = 'red',\n",
    "    alpha=1.0               # Transparency for better visibility\n",
    ")\n",
    "plt.xlim(0, 100)  \n",
    "plt.ylim(0,100)\n",
    "\n",
    "#ax.legend_.remove()  # Remove legend\n",
    "\n",
    "S_HDT_pearsonR_acc_conf = pearsonr(merged['SHDT_Mean_Confidence'], merged['Pred_Accuracy'])\n",
    "print('S_HDT: Is confidence related to how accurate they are? \\n R =', S_HDT_pearsonR_acc_conf[0], '\\n P-value:',S_HDT_pearsonR_acc_conf[1], '\\n R2 = ', S_HDT_pearsonR_acc_conf[0]**2 )\n",
    "\n",
    "\n",
    "# Create scatter plot 1\n",
    "plt.figure(figsize=(10, 6))\n",
    "norm = plt.Normalize(0, 1)\n",
    "cbar_ticks = np.arange(0, 1.1, 0.1) \n",
    "fig, ax = plt.subplots()\n",
    "scatter = sns.scatterplot(\n",
    "    data=merged, \n",
    "    x='Pred_PSS_Value', \n",
    "    y='SHDT_Mean_Confidence',\n",
    "    #size='Pred_PSS_Value',  # Scale dot size\n",
    "    #hue='Participant_ID',    # Color by participant\n",
    "    #palette='viridis',\n",
    "    #hue_norm=(0, 1),\n",
    "    color = 'red',\n",
    "    alpha=1.0               # Transparency for better visibility\n",
    ")\n",
    "plt.xlim(95, 405)  \n",
    "plt.ylim(0,100)\n",
    "\n",
    "#ax.legend_.remove()  # Remove legend\n",
    "\n",
    "S_HDT_pearsonR_pss_conf = pearsonr(merged['SHDT_Mean_Confidence'], merged['Pred_PSS_Value'])\n",
    "print('S_HDT: Is confidence related to their PSS? \\n R =', S_HDT_pearsonR_pss_conf[0], '\\n P-value:',S_HDT_pearsonR_pss_conf[1], '\\n R2 = ', S_HDT_pearsonR_pss_conf[0]**2 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848b03af-80dd-4f6a-a868-d4c0b1ca1677",
   "metadata": {},
   "source": [
    "# Training: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12d77b0-ab1b-4d63-844e-a727cb0b927f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_directory = '/PATH/SUBJECT_PATH/'\n",
    "training_list = []  # List to store individual DataFrames\n",
    "training_df = pd.DataFrame()\n",
    "for filename in os.listdir(data_directory):\n",
    "    if filename.endswith('.tsv'):  # Process only TSV files\n",
    "        # Extract participant ID: Keep only the numeric part (e.g., 'sub-005d' â†’ '005')\n",
    "        match = re.search(r'sub-(\\d+)', filename)\n",
    "        if not match:\n",
    "            continue  # Skip files that do not match the expected format\n",
    "        participant_id = match.group(1)  # Extract the numeric part (e.g., '005')\n",
    "        #print(participant_id)\n",
    "        # Read the participant's data file\n",
    "        participants_df = pd.read_csv(os.path.join(data_directory, filename), sep='\\t',  skiprows=1)  \n",
    "        participants_df.columns = [col.strip().replace('\\t', '') for col in participants_df.columns]\n",
    "    \n",
    "        participants_df = participants_df[participants_df['Staircase_name'] == 'Training']\n",
    "        participants_df['Participant_ID'] = participant_id\n",
    "         # Append to the list\n",
    "        training_list.append(participants_df)\n",
    "\n",
    "# Combine all DataFrames into a single one\n",
    "training_df = pd.concat(training_list, ignore_index=True)\n",
    "training_df['Trial'] = training_df['Trial'].astype(int)\n",
    "\n",
    "#print(training_df)\n",
    "\n",
    "print('TRAINING: How many trials were completed: \\n', training_df.groupby('Participant_ID')['Trial'].max().describe())\n",
    "print('Median: \\n', training_df.groupby('Participant_ID')['Trial'].max().median())\n",
    "\n",
    "\n",
    "print('\\n TRAINING: What delays were used: \\n', training_df['Current Delay'].describe())\n",
    "print('Median:', training_df['Current Delay'].median())\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.histplot(training_df['Current Delay'], bins=100, kde=True, color='#4C72B0', alpha=0.6)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print('TRAINING: How confident were people: \\n', training_df.groupby('Participant_ID')['Confidence'].max().describe())\n",
    "print('Median: \\n', training_df.groupby('Participant_ID')['Trial'].max().median())\n",
    "\n",
    "# Create the main histogram\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.histplot(training_df['Confidence'], bins=100, kde=True, color='#4C72B0', alpha=0.6)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3c0ea4-e1c2-448d-ba06-19f54b4922a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Scatter plot of AUC_x vs AUC_y\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x='AUC_x', y='AUC_y', data=aucs_merged, s=100)\n",
    "\n",
    "# Adding labels and title\n",
    "plt.xlabel('AUC_sHDT')\n",
    "plt.ylabel('AUC_HDT')\n",
    "plt.title('Scatter Plot of AUC_x vs AUC_y by Participant ID')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n",
    "\n",
    "print(aucs_merged)\n",
    "insight_pearsonR = pearsonr(aucs_merged['AUC_x'], aucs_merged['AUC_y'])\n",
    "print('S_HDT: Is AUC_SHDT related to AUC_HDT on task? \\n R =', insight_pearsonR[0], '\\n P-value:',insight_pearsonR[1], '\\n R2 = ', insight_pearsonR[0]**2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e579267-e6a0-4579-b88e-e765357756af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "null_per_trial_diff_df = pd.DataFrame()\n",
    "\n",
    "for i in tqdm(range(0,100)):  \n",
    "    participants_file = f'/PATH/SIMULATED_NULL/10000_random_participant_simulations_{i}.tsv'\n",
    "    all_null_df = pd.read_csv(participants_file, sep='\\t')\n",
    "\n",
    "    # Get participant IDs\n",
    "    participant_ids = []\n",
    "    participant_ids = all_null_df['Participant_ID'].unique().tolist()\n",
    "    #print(participant_ids)\n",
    "        \n",
    "    # Loop over each participant ID\n",
    "    for participant_id in participant_ids[0:100]:\n",
    "        participants_df = all_null_df[all_null_df['Participant_ID'] == participant_id]\n",
    "        #print(participants_df)\n",
    "        participants_df['Trial'] = participants_df['Trial'].astype(int)\n",
    "        participants_df['Modified_Staircase_name'] = participants_df['Staircase_name'].str[:-2]\n",
    "\n",
    "        # Recode trial numbers\n",
    "        participants_df.loc[:, 'Recode_Trial'] = np.where(\n",
    "            participants_df['Staircase_name'].str.contains('400'),\n",
    "            participants_df['Trial'], \n",
    "            participants_df['Trial'] + 15\n",
    "        )\n",
    "\n",
    "        #print(participants_df)\n",
    "\n",
    "         # For 400ms (trials 1-15)\n",
    "        mean_delays_400 = participants_df[participants_df['Staircase_name'].str.contains('400')] \\\n",
    "                            .groupby('Recode_Trial')['Current Delay'].mean()\n",
    "\n",
    "        # For 100ms (trials 16-30)\n",
    "        mean_delays_100 = participants_df[participants_df['Staircase_name'].str.contains('100')] \\\n",
    "                            .groupby('Recode_Trial')['Current Delay'].mean()\n",
    "\n",
    "        # Combine the two mean delay series (400_1, 400_2, and 100_1, 100_2) into one DataFrame\n",
    "        mean_delays = mean_delays_400.append(mean_delays_100)\n",
    "\n",
    "        # Reindex the trials: 1-15 for 400ms, 16-30 for 100ms\n",
    "        mean_delays.index = np.arange(1, 31)\n",
    "                \n",
    "        participant_data = {'Participant_ID': participant_id}\n",
    "        \n",
    "        # Compute differences for each trial\n",
    "        for trial in range(1, 16):\n",
    "            value_400 = mean_delays[trial]\n",
    "            value_100 = mean_delays[trial + 15]\n",
    "            trial_diff = value_400 - value_100\n",
    "\n",
    "            # Store values in dictionary\n",
    "            #participant_data[f'trial_{trial}_start'] = start_value\n",
    "            #participant_data[f'trial_{trial}_end'] = end_value\n",
    "            participant_data[f'trial_{trial}_diff'] = trial_diff\n",
    "\n",
    "        # Append the dictionary as a single row to the DataFrame\n",
    "        null_per_trial_diff_df = null_per_trial_diff_df.append(participant_data, ignore_index=True)\n",
    "\n",
    "\n",
    "display(HTML(null_per_trial_diff_df.head().to_html()))       "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
